%!TEX root = main.tex
\section{Model and Semantic Features}\label{sec:model}

\subsection{Model}
In this section we detail our problem model and the necessary background 
for our solution.

The problem we aim to solve is related to the Document Classification task 
\cite{attention,Slonim:2002:UDC:564376.564401,DBLP:journals/tkde/IsaLKR08} 
and tries to infer the direction the dialog is headed. 
In our model, we consider a dialog between two sides (a chatbot and a customer, two friends, etc.). 
A dialog can be modeled as a sequence of turns, 
where each turn is associated with a timestamp and a speaker, and can consist of several sentences. 
Therefore, we denote the set of all sequences of 
such turns by $\mathcal{T}$ and define the concepts of {\em sequence prefix} and {\em full dialog}. 

\begin{definition}
For $seq_1, seq_2 \in \mathcal{T}$, we say that $seq_1$ is the 
prefix of $seq_2$ and denote by $seq_1 = pr(seq_2)$, if 
$seq_1 = (t_1, \ldots, t_j)$, $seq_1 = (t_1, \ldots, t_j, \ldots, t_n)$, i.e., $seq_1$ contains the first 
turns in $seq_2$. 

We say that $seq \in \mathcal{T}$ is a full dialog if $\;\forall seq' \in \mathcal{T}. seq \neq pr(seq')\; $ 
\end{definition}

A {\em full dialog} is then a sequence $seq \in \mathcal{T}$ that is not a prefix 
of any other $seq' \in \mathcal{T}$. 
We then define an instance of {\em dialog-end}. 

\begin{definition}\label{def:target}
Given an integer $k$, an instance of dialog-end is a set of sequences of turns $T=\{seq_1, \ldots, seq_n\} \subseteq \mathcal{T}$ such that 
(1) each turn $t_i$ is associated with a timestamp and a speaker, (2) for every sequence $seq_i\in T$, there exists a full dialog $seq_j\in \mathcal{T}$ such that $seq_i = pr(seq_j)$, and $|seq_i| = k$ for all $1\leq i \leq n$.
\end{definition}

\begin{example}\label{ex:seq}
Consider the following sequence of turns from one chat in the Frames dataset \cite{frames}, where a chatbot tries to assist the user in booking a vacation. Here, $k=6$:
\begin{table}[!htb]
\centering
\begin{tabular}{| c | c | l |}
\hline Turn & Speaker & Text \\
\hline 1 & user & \begin{tabular}[x]{@{}l@{}}Hi. I need to book a vacation to Long Beach between August 25 \\and September 3. Departure is from Paris\end{tabular} \\
\hline 2 & bot & Would you like to depart Paris on September 6th? \\
\hline 3 & user & Preferably by the 3rd. Is there anything available? \\
\hline 4 & bot & \begin{tabular}[x]{@{}l@{}}Sorry, looks like there is nothing available from Paris to Long Beach on \\September 3rd.\end{tabular} \\
\hline 5 & user & \begin{tabular}[x]{@{}l@{}}I'm not quite sure I understand, is there anything available leaving \\Long Beach to go to Paris between August 25 and September 3rd?\end{tabular} \\
\hline 6 & bot & Would you like to depart Paris on September 6th? \\
\hline
\end{tabular}
%\vspace{-2mm}
% \caption{Notations}\label{notations}
% \vspace{-3mm}
\end{table}
\end{example}

The full dialogs we consider are targeted at a specific purpose 
(whether it be convincing the customer to purchase a product, helping a friend accomplish a task, etc.), 
and thus can be defined by their outcome, which 
is either success of failure (the customer buying the product, the friend accomplished the task, etc.). 
This concept can be modeled as another feature associated with the full dialog, where 1 means 
that the full dialog is successful and 0 means that it is not. 

\begin{example}
Reconsider the sequence in Example \ref{ex:seq}.  
If by the end of the full dialog, whose prefix is the mentioned sequence, 
the user books a vacation, the dialog is considered successful.
\end{example}
% Thus, we define the {\em dialog-end problem} as a target function, intuitively predicting the outcome of the chat from its prefix.

% \begin{definition}\label{def:target}
% Given an integer $k$ and a set of sequences of turns $T=\{seq_1, \ldots, seq_n\} \subseteq \mathcal{T}$,
% where each turn $t_i$ is associated with a timestamp and a speaker, for every sequence $seq_i\in T$, there exists a full dialog $seq_j\in \mathcal{T}$ such that $seq_i = pr(seq_j)$, and $|seq_i| = k$ the solution to the dialog-end 
% problem is a function $f:T \to \{0,1\}$ where for each sequence $seq_i$, $f(seq_i) = 1$ iff the $seq_j$ is successful. 
% \end{definition}

Our task is then a binary classification 
problem of finding a function that predicts the outcome 
of a full dialog $seq_j$ from its prefix $seq_i = pr(seq_j)$. 

Note that for every choice of $k$ we get a different 
version of the problem and a larger $k$ leads 
to a supposedly easier problem, since a larger sequence contains 
more information from the dialog, until the elements of $T$ 
are full dialogs. When the latter case is 
given, this can be interpreted as challenge in the Dialog Act field \cite{cs-CL-0006023,DBLP:conf/icassp/JiB05,DBLP:conf/coling/WermterL96} and can be solved using similar approaches, since we 
can tag all actions and search for the action signaling the task's completion. 
In our setting, however, we assume that $k$ is smaller than the dialog 
length, thus presenting a different challenge, requiring a somewhat different approach. 

We use neural language model features to leverage corpus
level word distributions and sentence understandings, 
specifically longer term
sequence probabilities. 
We elaborate on this in Section \ref{sec:network}. 
We further use two more types of features: auxiliary features 
extracted form the meta-data of the dialog and semantic features 
which employ an external knowledge base.


\subsection{Semantic Features}\label{sec:sem} 
In this part we describe the semantic features added 
to the representation of a sequence in our network. 
There are two types of features: meta features based on 
data extracted from the text itself and knowledge base features 
based on an external knowledge base.
% We use two types of features, designed to capture 
% different aspects of the problem. 
% We use the meta-data of 
% the dialog in order to extract characteristic features of the dialog 
% to help capture other aspects. 


% \paragraph*{Language Model Features}
% \amir{Efrat - Complete}
% A basic approach in this model of dialog is 
% using the semantic feature expressed in the text. 
% We use the GloVe \cite{glove} encoder to translate the words 
% into vectors and then use the network to extract sentences and 
% dialog vectors (see Section \ref{sec:network}). 



% \paragraph*{Auxiliary Semantic Features}\label{sec:semantic}
% Including additional features 
% extracted from the meta-data of the dialog can further enrich the our model. 
\paragraph*{Meta Features}
We add the following auxiliary features 
to describe the general nature 
of the interaction between the two sides 
in the sequence. All features here 
are computed from the given meta-data of the sequence and 
are part of the final representation of the data as it goes into the 
third level of the network (see Section \ref{sec:network}). 
Some of the features we add are related to the domain of our dataset 
(users trying to book a vacation \cite{frames}).

\begin{enumerate}
\item {\bf Time between turns: } the time between 
turns of the two sides, i.e. how long does it take to respond or ask a follow-up 
question after a given sentence on average. 
If responses are fast, it can imply that one side is dissatisfied 
from the other and does not have to think of her replies or whether 
she should agree with the other side.\label{itm:between}

% \item {\bf Total time of the dialog: } the total time of the interaction. \label{itm:total}

\item {\bf Turn length for each speaker: } since there are two sides to the dialog, 
we take the length of the turn for each of the two sides. 
A turn that contains more words means a more elaborate description, which indicates 
a good first impression.\label{itm:sent}


% \item {\bf Monetary Mentions: } if the topic of budget or money is mentioned in the dialog, 
% Intuitively, this feature indicates a progress in the booking process.\label{itm:budget}

% \item {\bf Dominant speaker: } Who is the speaker who is associated with the majority of sentences in the sequence 
\end{enumerate} 

\paragraph*{Knowledge Base Features}\label{sec:kb}
For these features, we employ the domain knowledge of our goal-oriented dialogs, 
namely, users trying to book a vacation \cite{frames}. 
% \amir{Slava - talk about semantic difference between sentences}
We use an external knowledge base to add more semantic features to our model. 
Specifically, we use a publicly available ontology (DBPedia \cite{dbpedia}) 
to attach three additional geographical-based features to each turn in the sequence. 
Given a sequence with $k$ turns, for each turn, we compute these three features 
based on the previous turn. 

\begin{enumerate}
\item {\bf Whether the cities are near-by: } if the previous turn included a mention of a city that is near to the city mentioned in the current turn (this is done for both the city of origin and the destination).
\item {\bf Whether the cities are from the same country: } if the previous turn included a mention of a city that is in the same country as the city mentioned in the current turn (this is done for both the city of origin and the destination).
\item {\bf Whether the cities are from the same continent: } similarly to the previous two features, checking whether the cities are in the same continent.
\end{enumerate}

Intuitively, if the user wants to travel to a city, 
and gets a suggestion about a near-by city, 
she is more likely to agree. 
On other hand, if we suggest a city that is far away, 
but still in the same country, she might agree. 
However, a suggestion of a city which is not even 
in the same country will probably not be accepted. 

In addition to geographical distance, we add time distance. 
We measure it as the difference in days between the requested and proposed dates. 
Usually people plan vacations and business meetings, and can have a few days change in the dates, 
but a change of a week or more of the travel time means less chances of booking.
Both features are based on expert knowledge about nowadays trends and reality about travel (especially with the new low-cost companies which has flight on specific dates, to certain places). 

% \begin{example}
% Consider a user who wishes to travel to Berlin on August 10. 
% If she is offered to book a flight to Munich on August 15 she is 
% likely to be more interested than if she is offered a flight to 
% Beijing on September 10. 
% \end{example}
