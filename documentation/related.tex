%!TEX root = main.tex
\section{Related Work}\label{sec:related}
There is a large body of work regarding dialog 
understanding in the context of chatbots and answer phrasing 
\cite{Jia09,ShawarA03,ShawarA08,BanchsK14,iris,noGoal,QiuLWGCZCHC17,YuXBR16,yu2016chatbot}. 
Works including \cite{iris,noGoal} use information 
retrieval to infer the appropriate responses based on 
semantic and/or syntactic similarity of the 
current chat to previous chats (stored in a database).  
This approach is augmented in \cite{QiuLWGCZCHC17}, 
where attention \cite{BahdanauCB14} was used in order 
rank the most relevant answers. 
Importantly, these systems rely on a database of previous 
chats in order to compute the appropriate responses where as 
our system relies solely on a model that tries to understand 
the current chat. 
In contrast, \cite{YuXBR16} uses reinforcement learning 
to design a policy to select designed
conversation strategies in non-goal-oriented dialog
systems, while also employing turn-level perspective. 
Our hierarchical representation 
that includes attention analyzes the entire dialog up to the last 
given turn and focuses specifically on words and sentences that 
have prominent affect on the direction of the dialog.

Dialog act \cite{DBLP:conf/icassp/JiB05,DBLP:conf/coling/WermterL96,AngLS05,SurendranL06,li2016multi,ortega2017neural} 
is a related field, focusing on the 
the characterization of parts or sentences in the dialog 
in terms of meaning. Recent works \cite{li2016multi,ortega2017neural} use 
deep learning and attention approaches to achieve accurate results. 
Naturally, when the full dialog is given as input, the dialog-end task 
can be viewed as a dialog act task since classifying the dialog act 
of each turn reveals whether the goal of the dialog was met. 
However, in the motivating use-case, the input is not a full dialog, and 
therefore, different approach is called for. 

Our system mostly relied on work 
done in the field of document classification \cite{attention,...} \amir{cite from attention paper} where input documents need to be classified into subjects 
(e.g., science, art, history etc.). 
Several learning approaches have been suggested here, 
such as n-grams \cite{...}, deep fully-connected networks \cite{...}, 
and convolutional networks \cite{...}. The recent approach \cite{attention} 
which seem to our problem as well relies on a hierarchical structure to 
encode the words, the sentences, and finally the whole document 
using GRUs and an attention mechanism. 
This approach relies on text alone to infer the class 
of the document, while we also add semantic features to better 
understand the context and add a layer above the sentence layer 
to encode the turns. 
