%!TEX root = main.tex
\section{Network Architecture}\label{sec:network}

\def\layersep{1.2cm}
\def\layerseps{\layersep + \layersep + \layersep + \layersep}
\def\layersept{\layerseps + \layerseps}
\def\layersepst{\layersept + \layerseps}
\begin{figure}
\begin{center}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep, scale=1]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{semantic neuron}=[neuron, fill=yellow!50];
    \tikzstyle{annot} = [text width=5em, text centered]

    % Draw the input layer nodes
    % \foreach \name / \y in {1,...,4} 
    % % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
    %     \node[input neuron] (I-\name) at (0,-\y) {};% , pin=left:Input \#\y

    \foreach \name / \y in {1,...,4}{
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron] (I-\name) at (0,-\y) {$w_{2\y}$}; 
        \node[input neuron] (S-\name) at (\layerseps,-\y) {$s_{\y}$}; 
        \node[input neuron] (T-\name) at (\layersept,-\y) {$t_{\y}$}; 
        % \node[input neuron] (ST-\name) at (\layersepst,-\y) {$st_{\y}$}; 
        }

    \foreach \name / \y in {1,...,4}{
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[hidden neuron] (H-\name) at (\layersep,-\y cm) {$\vec{h}_{2\y}$}; 
        \node[hidden neuron] (H-sent-\name) at (\layerseps + \layersep,-\y cm) {$\vec{h}_{3\y}$};
        \node[hidden neuron] (H-turn-\name) at (\layersept + \layersep,-\y cm) {$\vec{h}_{\y}$};
        % \node[hidden neuron] (H-sem-turn-\name) at (\layersepst + \layersep,-\y cm) {$\vec{h}_{\y}$};  
        }


    \foreach \name / \y in {1,...,4}{
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[hidden neuron] (H-rev-\name) at (\layersep + \layersep,-\y cm) {$\cev{h}_{2\y}$};
        \node[hidden neuron] (H-rev-sent-\name) at (\layerseps + \layersep + \layersep,-\y cm) {$\cev{h}_{3\y}$};
        \node[hidden neuron] (H-rev-turn-\name) at (\layersept + \layersep + \layersep,-\y cm) {$\cev{h}_{\y}$};
        % \node[hidden neuron] (H-rev-sem-turn-\name) at (\layersepst + \layersep + \layersep,-\y cm) {$\cev{h}_{\y}$};
        }

    \foreach \name / \y in {1,...,4}{
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[] (A-\name) at (\layersep + \layersep + \layersep,-\y-0.2) {$\alpha_{3\y}$};
        \node[] (A-sent-\name) at (\layerseps + \layersep + \layersep + \layersep,-\y-0.2) {$\alpha_{\y}$};
        \node[] (A-turn-\name) at (\layersept + \layersep + \layersep + \layersep,-\y-0.2) {$\alpha_{\y}$};
         % \node[] (A-sem-turn-\name) at (\layersepst + \layersep + \layersep + \layersep,-\y-0.2) {$\alpha_{\y}$};
        }


    \node[output neuron, right of=A-turn-3] (V) at (\layersept + \layersep + \layersep + \layersep + .4cm,-2.3)  {$\vec{v_d}$};
    % \node[output neuron, right of=V] (sm) {softmax};

    \node[output neuron, above of=H-rev-1] (U) {$u_w$};
    \node[output neuron, above of=H-rev-sent-1] (Us) {$u_s$};
    \node[output neuron, above of=H-rev-turn-1] (Ut) {$u_t$};
    \node[semantic neuron] (sem) at (\layerseps + \layersep + \layersep + \layersep, 0.3) {$u_{sem}$};% 
    % \node[semantic neuron] (sem) at (\layersept + \layersep+\layersep+\layersep,0.3) {$u_{sem}$};% 

   
    % \node[output neuron, right of=H-3] (O) {};
    % Draw the output layer node
    % \node[text centered] (D) at (\layersept + \layerseps + \layersep,-2.2) {dense};
    \node[text centered, right of=V] (O) {softmax};%, pin={[pin edge={->}]right:Output}
    \path (V) edge (O); %at (\layersept + \layerseps + \layersep + \layersep + 0.5cm,-2.2) 

     \foreach \source in {1,...,4}{
     	\path (sem) edge (T-\source);
     }

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,4}{
        \path (I-\source) edge (H-\source);
        \path (S-\source) edge (H-sent-\source);
        \path (T-\source) edge (H-turn-\source);
        % \path (ST-\source) edge (H-sem-turn-\source);
        }

     \foreach \source [count=\i from 2] in {1,...,3}{
        \path (H-\source) edge (H-\i);
        \path (H-sent-\source) edge (H-sent-\i);
        \path (H-turn-\source) edge (H-turn-\i);
        % \path (H-sem-turn-\source) edge (H-sem-turn-\i);
        }

    \foreach \source [count=\i from 2] in {1,...,3}{
        \path (H-rev-\i) edge (H-rev-\source) ;
        \path (H-rev-sent-\i) edge (H-rev-sent-\source);
        \path (H-rev-turn-\i) edge (H-rev-turn-\source);
        % \path (H-rev-sem-turn-\i) edge (H-rev-sem-turn-\source);
        }

    

    \foreach \source in {1,...,4}{
    	\node[draw,dotted,thick,fit=(H-rev-\source) (H-\source)] (box-\source) {};
    	\node[draw,dotted,thick,fit=(H-rev-sent-\source) (H-sent-\source)] (box-sent-\source) {};
    	\node[draw,dotted,thick,fit=(H-rev-turn-\source) (H-turn-\source)] (box-turn-\source) {};
    	% \node[draw,dotted,thick,fit=(H-rev-sem-turn-\source) (H-sem-turn-\source)] (box-sem-turn-\source) {};
    	}

   	\foreach \source in {1,...,4}{
        \path (box-\source) edge (A-\source);
        \path (U) edge (A-\source);

        \path (box-sent-\source) edge (A-sent-\source);
        \path (Us) edge (A-sent-\source);

        \path (box-turn-\source) edge (A-turn-\source);
        \path (Ut) edge (A-turn-\source);
        }

   	\foreach \source in {1,...,4} {
    	\path (box-\source) edge (S-2);
    	\path (A-\source) edge (S-2);

    	\path (box-sent-\source) edge (T-3);
    	\path (A-sent-\source) edge (T-3);

    	% \path (box-turn-\source) edge (ST-2);
    	% \path (A-turn-\source) edge (ST-2);

    	\path (box-turn-\source) edge (V);
    	\path (A-turn-\source) edge (V);
    	% \path (box-sem-turn-\source) edge (V);
    	% \path (A-turn-\source) edge (V);
    	}

    	% \path (sem) edge (V);
    	% \path (V) edge (D);




    % % Connect every node in the hidden layer with the output layer
    % \foreach \source in {1,...,4}
    %     \path (H-\source) edge (O);

    % Annotate the layers
    \node[annot,below of=box-4, node distance=1.1cm] (we) {Word Encoder};
    \node[annot,below of=A-4, node distance=.9cm] (wa) {Word Attention};

    \node[annot,below of=box-sent-4, node distance=1.1cm] (se) {Sent. Encoder};
    \node[annot,below of=A-sent-4, node distance=.9cm] (sa) {Sent. Attention};

    \node[annot,below of=box-turn-4, node distance=1.1cm] (te) {Turn Encoder};
    \node[annot,below of=A-turn-4, node distance=.9cm] (ta) {Turn Attention};

    % \node[annot,below of=box-sem-turn-4, node distance=1.1cm] (te) {Sem. Turn Encoder};
    % \node[annot,left of=hl] {Input layer};
    % \node[annot,right of=hl] {Output layer};
\end{tikzpicture}
\caption{Network Architecture}\label{fig:arch}
\end{center}
\end{figure}
% For the architecture of our network , 
Our network (whose architecture is depicted in Figure \ref{fig:arch}) consists of two main components: 
a text encoder for which we have augmented the hierarchical attention network inspired by \cite{attention}, 
and a semantic feature encoder. 

\subsection{Text Encoder}

\paragraph*{LSTM Encoder}
We have divided the task of encoding the dialog into several 
layers of encoders, each dealing with the dialog in a different granularity. 
At the lowest lever, we encode the words of each dialog into vectors 
and use word-level attention. Then a sentence encoder and sentence-level attention 
translate the sentences into vectors, again using sentence-level attention, to extract a vector describing the text 
in each turn. Finally, we use the same mechanism of turn-level encoder and attention, where each turn is combined 
with its semantic features (Section \ref{sec:sem}). 
For the first step of encoding in each level, we use an LSTM-based sequence encoder \cite{lstm,birnn}. 
% We use a LSTM-based sequence encoder \cite{BahdanauCB14} which uses 
% a reset gate and an update gate ($r_t$ and $z_t$, respectively), 
% so that at time $t$ the state of the LSTM is 

% \begin{equation*}
% 	h_t = (1-z_t)\cdot h_{t-1} + z_t \cdot \widetilde{h_t}
% \end{equation*}

% Where $h_{t-1}$ is the previous state and $\widetilde{h_t}$ 
% is the current new state, computed with the new sequence information. 
% $z_t$ controls the amount of past information that is kept 
% and the amount of newly added information. $z_t$ is updated like so

% \begin{equation*}
% 	z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z)
% \end{equation*}

% $x_t$ is the sequence vector at time $t$ and 
% $\widetilde{h_t}$ is the candidate state, which is 
% computed in this manner: 

% \begin{equation*}
% 	\widetilde{h_t} = tanh(W_h x_t + r_t \cdot U_h h_{t-1} + b_h)
% \end{equation*}

% As mentioned previously, $r_t$ is the reset gate 
% controlling how much of the information embedded in 
% the previous state will contribute to the new candidate state. 
% Note that if $r_t = 0$, the previous state is neglected. 
% To update $r_t$, we have the following equation (similar to the update of $z_t$)

% \begin{equation*}
% 	r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r)
% \end{equation*}

\paragraph*{Combining Attention}
Given an instance of dialog-end $T=\{seq_1, \ldots, seq_n\} \subseteq \mathcal{T}$, where $|seq_i| = k$ for some $k \in \mathbb{N}$, each sequence contains $S_i$ sentences, each sentence $s_i \in seq_j$ contains $T_i$ words. 
We describe the different levels of encoding the attention that lead 
to an encoding of the entire sequence. From there, we add the semantic features and perform 
a binary classification. 

In the word level, we embed each word using an embedding matrix \cite{glove} 
and use a bidirectional LSTM 
to get annotations of words by summarizing information
from both directions for words (Word Encoder component in Figure \ref{fig:arch}). 
Employing both a forward and a backward LSTM 
allows the encoder to get the contextual meaning of the words. 
For a given word $w_{it}, t\in [0,T_i]$, 
we concatenate the forward hidden state and
backward hidden state to get its annotation. 

In the attempt of distinguishing the important 
words in the sentence from the rest, we employ word-level 
attention (Word Attention component in Figure \ref{fig:arch}). We highlight the words that most contribute 
to the meaning of the entire sentence so that we can represent 
a sentence vector $s_i$. 

First, the word annotation $h_{it}$ is fed 
through a one-layer MLP to get $u_{it}$ as a hidden representation
of $h_{it}$. Then, the normalized importance weight of the word $\alpha_{it}$ 
is computed as the similarity of $u_{it}$ with the word-level 
context vector $u_w$. 
Finally, the sentence vector $s_i$ is a weighted sum of the word annotations
based on the weights. 

\begin{equation*}
	u_{it} = tanh(W_w h_{it} + b_w)
\end{equation*}

\begin{equation*}
	\alpha_{it} = \frac{exp(u^T_{it} u_w)}{\sum_t exp(u^T_{it} u_w)}
\end{equation*}

\begin{equation*}
	s_i = \sum_{t} \alpha_{it} h_{it}
\end{equation*}

We use a similar mechanism (including the forward and backward 
LSTM, and the attention layer) for the sentence-level vectors (Sent. Encoder and Sent. Attention components in Figure \ref{fig:arch}), 
and for the turns in the sequence $seq_i$, in order to get a sequence representation vector 
(Turn Encoder and Turn Attention components in Figure \ref{fig:arch}). 
Before applying the LSTM and attention on the turns, we add the corresponding semantic features 
of each turn. 
% The output of this stage is a turn vector, representing {\em the text} in the each turn in the sequence. 

\subsection{Semantic Features}
The additional semantic features specified 
in Section \ref{sec:sem} ($u_{sem}$ in Figure \ref{fig:arch})
are added as coordinates to each of the turn vectors, 
so that each vector also contains semantic features  ($t_i$, $1\leq i \leq 4$ in Figure \ref{fig:arch}). 
All features are normalized by taking the largest 
value in each coordinate across all sequences and dividing 
all values in that coordinate by this value. 

These turn vectors are then inserted into a bidirectional RNN whose output 
is a vector representation of the entire sequence ($\vec{v_d}$ in Figure \ref{fig:arch}). 
The generated vector is the final encoding for the sequence.


% Note that the length of the vector depends on $k$, 
% i.e., the number of sentences in the sequences. 
% A semantic vector has $2k+3$ coordinates since feature \ref{itm:between} in Section \ref{sec:sem} requires $k-1$ coordinates, and 
% feature \ref{itm:sent} requires $k$ coordinates. 
% features \ref{itm:total}, \ref{itm:budget} requires a single coordinate each, 
% and the knowledge based features are summarized into $2$ additional coordinates, 
% one for geographical similarity and the other for time similarity during the sequence. 

% This vector is then appended to the vector representing the encoded text in the sequence, 
% generated through the model mentioned above. 
% The generated vector $\vec{v_d}$ is the final encoding for the sequence.



% Once we have the representation of all the turns in $seq_i$, 
% we again use a LSTM to convert then into a sequence vector $\vec{v_d}$. 
% These vectors are the input for a fully-connected neural 
% network with 2 hidden layers, with 12 and 8 neurons, respectively. 
% As this is a binary classification, the network has an output layer of size 1. 
% For both hidden layers we have used the rectified linear unit as the activation 
% function and for the output we applied the sigmoid function.


\subsection{Classification}
% After obtaining the abstract representation of the sequences, 
% we use a a hidden fully connected layer with Relu activation function: 

% \begin{equation*}
% 	f(x) = max(0, x)
% \end{equation*}

After obtaining the abstract representation of the sequences, 
we then have a final output layer containing two neurons to classify the sequences:
% and generate a function approximating the solution (Definition \ref{def:target}) 

\begin{equation*}
	p = softmax(W_c v + b_c)
\end{equation*}

Negative log-likelihood was used to correct labels ($j$ is the label of sequence $i$): 

\begin{equation*}
	L = - \sum_{i} \log{p_{ij}}
\end{equation*}

