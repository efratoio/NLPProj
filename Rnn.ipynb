{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"./Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\",\")\n",
    "        d[k] = True if v.strip()==\"True\" else False\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gen_chat_data():\n",
    "    chat_path = r\"./Frames-dataset/chats\"\n",
    "    chats = {}\n",
    "    d = build_label_dict()\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        \n",
    "        turns = []\n",
    "        if 'turns' in chat:\n",
    "            tsp = chat['turns'][0]['timestamp']\n",
    "       \n",
    "            for turn in chat['turns']:\n",
    "               \n",
    "                ts = turn['timestamp'] - tsp\n",
    "                tsp = turn['timestamp']\n",
    "                turns.append({\"ti\":ts,\"text\":turn[\"text\"],\"author\":turn[\"author\"]})\n",
    "\n",
    "        chats[filename[:-5]] = {}\n",
    "        chats[filename[:-5]][\"turns\"] = turns\n",
    "        chats[filename[:-5]][\"label\"] = d[filename[:-5]]\n",
    "    return chats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/efrat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "import theano\n",
    "\n",
    "import functools\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "\n",
    "MAX_SENT_LENGTH = 40\n",
    "MAX_SENTS = 20\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_TURNS = 60\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.2\n",
    "RNN_DIM = 50\n",
    "GLOVE_DIR = \"./data/glove\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_texts_labels(chats,flag=False,length=0):\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    for idx in chats.keys():\n",
    "        if flag:\n",
    "        \n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"][:length]])\n",
    "        else:\n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"]])\n",
    "        texts.append(text)\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        labels.append(chats[idx][\"label\"])   \n",
    "    return texts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_with_turns(chats, flag=False,length=0):\n",
    "    texts = []\n",
    "    labels=[]\n",
    "    chats_txt=[]\n",
    "    vecs=[]\n",
    "    for idx in chats.keys():\n",
    "        if flag:\n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"][:length]])\n",
    "            \n",
    "        else:\n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"]])\n",
    "        turns=[]\n",
    "        vec=[]\n",
    "        for turn in chats[idx][\"turns\"]:\n",
    "            vec.append([len(turn[\"text\"]),turn[\"ti\"],0 if turn[\"author\"].lower()==\"wizard\" else 1])\n",
    "            texts.append(turn[\"text\"])\n",
    "            sentences = tokenize.sent_tokenize(turn[\"text\"])\n",
    "            turns.append(sentences)\n",
    "        vecs.append(vec)\n",
    "        chats_txt.append(turns)\n",
    "        labels.append(chats[idx][\"label\"])\n",
    "        \n",
    "\n",
    "    return vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(chats,flag=False, length =0):\n",
    "    if flag:\n",
    "        chats = {k: v for k, v in chats.items() if len(v[\"turns\"])>length}\n",
    "    \n",
    "    texts,labels = create_texts_labels(chats,flag,length)\n",
    "    vecs = create_data_with_turns(chats, flag,length)\n",
    "\n",
    "    aux_data = np.zeros((len(texts), MAX_TURNS,3), dtype='float32')\n",
    "    for i,vec in enumerate(vecs):\n",
    "        if flag:\n",
    "            for j,v in enumerate(vec[:length]):\n",
    "                aux_data[i,j,:]= np.array(v)\n",
    "        else:\n",
    "            for j,v in enumerate(vec):\n",
    "                aux_data[i,j,:]= np.array(v)\n",
    "    \n",
    "    \n",
    "    \n",
    "    norm_hlp = norm(aux_data,axis=2,ord=2)\n",
    "    aux_data = aux_data.astype(np.float)/norm_hlp[:,:,None]\n",
    "\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    print('Shape of aux tensor:', aux_data.shape)\n",
    "#     data = data[:13]\n",
    "#     labels = labels[:13]\n",
    "    indices = np.arange(len(aux_data))\n",
    "    np.random.shuffle(indices)\n",
    "    aux_data = aux_data[indices]\n",
    "\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * aux_data.shape[0])\n",
    "    nb_test_samples = int(TEST_SPLIT * aux_data.shape[0])\n",
    "\n",
    "    x_train = aux_data[:-(nb_validation_samples+nb_test_samples)]\n",
    "    y_train = labels[:-(nb_validation_samples+nb_test_samples)]\n",
    "    x_val = aux_data[-(nb_validation_samples+nb_test_samples):-nb_test_samples]\n",
    "    y_val = labels[-(nb_validation_samples+nb_test_samples):-nb_test_samples]\n",
    "    x_test = aux_data[-nb_test_samples:]\n",
    "    y_test = labels[-nb_test_samples:]\n",
    "    \n",
    "    return x_train, y_train,x_val, y_val,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model,x_train, y_train,x_val, y_val,x_test,y_test):\n",
    "    print(model.metrics_names)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "    nb_epoch=10, batch_size=10)\n",
    "\n",
    "    return model.evaluate(x_test,y_test,batch_size=10, verbose=1, sample_weight=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_network():\n",
    "\n",
    "    review_input = Input(shape=(MAX_TURNS,3), dtype='float32')\n",
    "    l_lstm = Bidirectional(LSTM(10))(review_input)\n",
    "\n",
    "\n",
    "    preds = Dense(2, activation='softmax')(l_lstm)\n",
    "    model = Model(review_input, preds)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  def run_net(net,file,x_train, y_train,x_val, y_val,x_test,y_test):\n",
    "    model = net()\n",
    "    res = evaluate_model(model,x_train, y_train,x_val, y_val,x_test,y_test)\n",
    "    file.write(str(net)+\"\\n\")\n",
    "    file.write(str(res)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (1368, 2)\n",
      "Shape of aux tensor: (1368, 60, 3)\n",
      "['loss', 'acc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 959 samples, validate on 136 samples\n",
      "Epoch 1/10\n",
      "959/959 [==============================] - 2s - loss: nan - acc: 0.4046 - val_loss: nan - val_acc: 0.3529\n",
      "Epoch 2/10\n",
      "650/959 [===================>..........] - ETA: 4s - loss: nan - acc: 0.3938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.190309). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.189120). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.157665). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.101910). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950/959 [============================>.] - ETA: 0s - loss: nan - acc: 0.4042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.205461). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959/959 [==============================] - 20s - loss: nan - acc: 0.4046 - val_loss: nan - val_acc: 0.3529\n",
      "Epoch 3/10\n",
      "959/959 [==============================] - 3s - loss: nan - acc: 0.4046 - val_loss: nan - val_acc: 0.3529\n",
      "Epoch 4/10\n",
      "120/959 [==>...........................] - ETA: 1235s - loss: nan - acc: 0.4167ETA: 230s - loss: nan - acc: 0.40 - ETA: 267s - loss: nan - acc: 0.43 - ETA: 460s - loss: nan - acc: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.472939). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959/959 [==============================] - 527s - loss: nan - acc: 0.4046 - val_loss: nan - val_acc: 0.3529 loss: nan - acc: 0.40 - ETA: 38s - loss: nan - acc - ETA: 18s - loss: nan - acc: 0.406 - ETA: 16s - loss: nan - acc:  - ETA: 3s - loss: nan - acc: 0.40\n",
      "Epoch 5/10\n",
      " 50/959 [>.............................] - ETA: 1574s - loss: nan - acc: 0.3800"
     ]
    }
   ],
   "source": [
    "chats = gen_chat_data()\n",
    "with open(\"rnnresults\",\"w\") as results_file:\n",
    "    x_train, y_train,x_val, y_val,x_test,y_test = prepare_datasets(chats)\n",
    "    run_net(rnn_network,results_file,x_train, y_train,x_val, y_val,x_test,y_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
