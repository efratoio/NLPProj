{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a label dictionery and collect features for chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/amirgilad/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "# from nltk.corpus import brown\n",
    "from nltk import tokenize\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse features of chats and adapt to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of sents in chat: 1.0069528415961306\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\"\\t\")\n",
    "        d[k] = v.strip()\n",
    "    return d\n",
    "\n",
    "def gen_chat_model(label_dict):\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chat_features = []\n",
    "    avg_chat_len = 0\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            for turn in chat['turns']:\n",
    "#                 if 'acts_without_refs' in turn['labels'] and 'name' in turn['labels']['acts_without_refs'][0]:\n",
    "#                     tokenizer.word_index[ turn['labels']['acts_without_refs'][0]['name'] ] # add type of request\n",
    "#                 print(turn['labels'])\n",
    "#                 tokenizer.word_index[ turn['labels']['acts'][0]['name'] ]\n",
    "                if len(turn['labels']['acts']) > 0:\n",
    "                    sents.append( turn['labels']['acts'][0]['name'].replace(\"_\", \" \") )\n",
    "                turn['timestamp']\n",
    "                avg_chat_len += 1\n",
    "        chat_features.extend( sents )\n",
    "    avg_chat_len /= len(chat_features)\n",
    "    print(\"average number of sents in chat: \" + str(avg_chat_len))\n",
    "    return chat_features\n",
    "\n",
    "d = build_label_dict()\n",
    "chat_features = gen_chat_model(d)\n",
    "tokenizer.fit_on_texts(chat_features) # fit model to labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vectors from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np \n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\"\\t\")\n",
    "        d[k] = v.strip()\n",
    "    return d\n",
    "\n",
    "def compute_sum_ts(prev_ts, cur_ts, sum_ts):\n",
    "    if cur_ts != 0:\n",
    "        sum_ts += cur_ts - prev_ts\n",
    "    return sum_ts\n",
    "\n",
    "def gen_vectors(d):\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chat_features = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_vec = np.zeros(4)\n",
    "        turns = 0\n",
    "        prev_ts, sum_ts = 0,0\n",
    "        first_ts = 0\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            for turn in chat['turns']:\n",
    "#                 if len(turn['labels']['acts']) > 0:\n",
    "#                     sents.append( turn['labels']['acts'][0]['name'].replace(\"_\", \" \") )\n",
    "                sum_ts = compute_sum_ts(prev_ts, turn['timestamp'], sum_ts)# compute avg time between questions and responses\n",
    "                prev_ts = turn['timestamp']\n",
    "                turns += 1\n",
    "                first_ts = turn['timestamp'] if first_ts == 0 else first_ts\n",
    "        chat_vec[0] = sum_ts / turns # avg time\n",
    "        chat_vec[1] = 0 # most common action\n",
    "        chat_vec[2] = prev_ts - first_ts # chat total time\n",
    "        chat_vec[3] = turns # number of turns\n",
    "        chat_features.append( chat_vec )\n",
    "        labels.append(d[filename])\n",
    "        \n",
    "    return np.array(chat_features), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6827 - acc: 0.6140     \n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6782 - acc: 0.6140     \n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6748 - acc: 0.6140     \n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6725 - acc: 0.6140     \n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6706 - acc: 0.6140     \n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6693 - acc: 0.6140     \n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6686 - acc: 0.6140     \n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6682 - acc: 0.6140     \n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6677 - acc: 0.6140     \n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6675 - acc: 0.6140     \n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6672 - acc: 0.6140     \n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6671 - acc: 0.6140     \n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6671 - acc: 0.6140     \n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6671 - acc: 0.6140     \n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6669 - acc: 0.6140     \n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6671 - acc: 0.6140     \n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6669 - acc: 0.6140     \n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6669 - acc: 0.6140     \n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6672 - acc: 0.6140     \n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6671 - acc: 0.6140     \n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6669 - acc: 0.6140     \n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6671 - acc: 0.6140     \n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     - ETA: 0s - loss: 0.6688 - acc: 0.6\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6673 - acc: 0.6140     \n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6671 - acc: 0.6140     \n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6669 - acc: 0.6140     \n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6669 - acc: 0.6140     \n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     - ETA: 0s - loss: 0.6678 - acc: 0.612\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6670 - acc: 0.6140     \n",
      " 32/369 [=>............................] - ETA: 0s\n",
      "acc: 60.43%\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, merge, Embedding\n",
    "from keras.models import Sequential\n",
    "TRAIN_SIZE = 1000\n",
    "INPUT_DIM = 4\n",
    "\n",
    "# Generate data\n",
    "d = build_label_dict()\n",
    "X, Y = gen_vectors(d)\n",
    "\n",
    "# Divide into training and test data\n",
    "x_train, y_train = X[:TRAIN_SIZE], Y[:TRAIN_SIZE]\n",
    "x_test, y_test = X[TRAIN_SIZE:], Y[TRAIN_SIZE:]\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=INPUT_DIM, activation='relu'))\n",
    "model.add(Dense(32, activation='softmax'))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and fit\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Test the model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7392876e3dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0msentence_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SENT_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0ml_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0ml_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m                 raise ValueError('Layer weight shape ' +\n\u001b[1;32m   1188\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# author - Richard Liao \n",
    "# Dec 26 2016\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "# data_train = pd.read_csv('~/Testground/data/imdb/labeledTrainData.tsv', sep='\\t')\n",
    "# print data_train.shape\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "# reviews = []\n",
    "# labels = []\n",
    "# texts = []\n",
    "\n",
    "# for idx in range(data_train.review.shape[0]):\n",
    "#     text = BeautifulSoup(data_train.review[idx])\n",
    "#     text = clean_str(text.get_text().encode('ascii','ignore'))\n",
    "#     texts.append(text)\n",
    "#     sentences = tokenize.sent_tokenize(text)\n",
    "#     reviews.append(sentences)\n",
    "    \n",
    "#     labels.append(data_train.sentiment[idx])\n",
    "\n",
    "# tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "# for i, sentences in enumerate(reviews):\n",
    "#     for j, sent in enumerate(sentences):\n",
    "#         if j< MAX_SENTS:\n",
    "#             wordTokens = text_to_word_sequence(sent)\n",
    "#             k=0\n",
    "#             for _, word in enumerate(wordTokens):\n",
    "#                 if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "#                     data[i,j,k] = tokenizer.word_index[word]\n",
    "#                     k=k+1                    \n",
    "                    \n",
    "# word_index = tokenizer.word_index\n",
    "# print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "# labels = to_categorical(np.asarray(labels))\n",
    "# print('Shape of data tensor:', data.shape)\n",
    "# print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# indices = np.arange(data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# data = data[indices]\n",
    "# labels = labels[indices]\n",
    "# nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "# x_train = data[:-nb_validation_samples]\n",
    "# y_train = labels[:-nb_validation_samples]\n",
    "# x_val = data[-nb_validation_samples:]\n",
    "# y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "# print('Number of positive and negative reviews in traing and validation set')\n",
    "# # print y_train.sum(axis=0)\n",
    "# # print y_val.sum(axis=0)\n",
    "\n",
    "# GLOVE_DIR = \"/ext/home/analyst/Testground/data/glove\"\n",
    "# embeddings_index = {}\n",
    "# f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                             EMBEDDING_DIM,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SENT_LENGTH,\n",
    "#                             trainable=True)\n",
    "\n",
    "# sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sentence_input)\n",
    "# l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "# sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "# review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "# review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "# preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "# model = Model(review_input, preds)\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc'])\n",
    "\n",
    "# print(\"model fitting - Hierachical LSTM\")\n",
    "# print (model.summary())\n",
    "# model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           nb_epoch=10, batch_size=50)\n",
    "\n",
    "# building Hierachical Attention network\n",
    "# embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(1000 + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[1],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "l_att = AttLayer()(l_dense)\n",
    "\n",
    "\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
