{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple word2vec model with 200 features for each utterance, not considering other features (speaker etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec model for all chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gensim\n",
    "\n",
    "# path = r\"C:\\Users\\user\\git\\NLPProj\\Frames-dataset\\chats\"\n",
    "\n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\"\\t\")\n",
    "        d[k] = v.strip()\n",
    "    return d\n",
    "\n",
    "def gen_chat_model(label_dict):\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chats = []\n",
    "    avg_chat_len = 0\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            for turn in chat['turns']:\n",
    "                sents.extend( turn['text'].split() )\n",
    "                avg_chat_len += 1\n",
    "        tagged_chat = gensim.models.doc2vec.TaggedDocument(sents, label_dict[filename])\n",
    "        chats.append( tagged_chat )\n",
    "    avg_chat_len /= len(chats)\n",
    "    print(\"average number of sents in chat: \" + str(avg_chat_len))\n",
    "    return gensim.models.Doc2Vec(chats, size=100, window=8, min_count=1, workers=4)\n",
    "\n",
    "# d = build_label_dict()\n",
    "# chat_model = gen_chat_model(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input vectors from first parts of chats and labels from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_vectors(chat_model, label_dict, num_of_sents_from_start):\n",
    "    '''infer vectors for every sentence in the chats. \n",
    "    Will be used to load the training data and tags'''\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chats_vecs = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            lim = min(num_of_sents_from_start, len(chat['turns']))\n",
    "            for i in range( lim ):\n",
    "                sents.extend( chat['turns'][i]['text'].split() )\n",
    "#             for turn in chat['turns']:\n",
    "#                 sents.extend( turn['text'].split() )\n",
    "        sent_vec = chat_model.infer_vector( sents )\n",
    "        chats_vecs.append( sent_vec )\n",
    "        labels.append(label_dict[filename])\n",
    "    return np.array(chats_vecs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot model for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_one_hot_dict():\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    index_dict = {}\n",
    "    i = 0\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        if 'turns' in chat:\n",
    "            for turn in chat['turns']:\n",
    "                for word in turn['text'].split():\n",
    "                    if word.strip() not in index_dict:\n",
    "                        index_dict[word.strip()] = i\n",
    "                        i += 1\n",
    "    return index_dict\n",
    "\n",
    "def infer_vectors_from_one_hot(index_dict, label_dict, num_of_sents_from_start):\n",
    "    '''infer vectors for every sentence in the chats. \n",
    "    Will be used to load the training data and tags'''\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chats_vecs = []\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            lim = min(num_of_sents_from_start, len(chat['turns']))\n",
    "            for i in range( lim ):\n",
    "                sents.extend( [index_dict[word.strip()] for word in chat['turns'][i]['text'].split()] )\n",
    "                \n",
    "        sent_vec = \n",
    "        chats_vecs.append( sent_vec )\n",
    "        labels.append(label_dict[filename])\n",
    "    return np.array(chats_vecs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM network - does not perform well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of sents in chat: 14.598977355734112\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6827 - acc: 0.5760    \n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6701 - acc: 0.6140     \n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6683 - acc: 0.6140     \n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6679 - acc: 0.6140    \n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6676 - acc: 0.6140     \n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6673 - acc: 0.6140     \n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6673 - acc: 0.6140     \n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 19s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 12s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 12s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 16s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 16s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 17s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 16s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 18s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 18s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 15s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 16s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 20s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 12s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 16s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6671 - acc: 0.6140    \n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 9s - loss: 0.6670 - acc: 0.6140     \n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 11s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 10s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 17s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 14s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 12s - loss: 0.6669 - acc: 0.6140    \n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 12s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 13s - loss: 0.6670 - acc: 0.6140    \n",
      "369/369 [==============================] - 1s     \n",
      "\n",
      "acc: 60.43%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, merge, Embedding, RNN\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import sys\n",
    "# should change to relative import\n",
    "sys.path.insert(0, '/home/amirgilad/Documents/git/NLPProj/kerasAttention')\n",
    "import attention_dense\n",
    "import attention_lstm\n",
    "\n",
    "input_dims = x_train.shape\n",
    "train_size = 1000\n",
    "\n",
    "# Build doc2Vec model for the chats\n",
    "d = build_label_dict()\n",
    "chat_model = gen_chat_model(d)\n",
    "\n",
    "# Load the data for the train and test\n",
    "# The last param is the num. of rows we take from the begining of the chat\n",
    "num_of_rows_from_start_of_chat = 6\n",
    "X, Y = infer_vectors(chat_model, d, num_of_rows_from_start_of_chat)\n",
    "\n",
    "# Divide into training and test data\n",
    "x_train, y_train = X[:train_size], Y[:train_size]\n",
    "x_test, y_test = X[train_size:], Y[train_size:]\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "# model.add(Dense(80, input_dim=100, activation='relu'))\n",
    "input_layer = Dense(80, input_dim=100, activation='relu')\n",
    "\n",
    "# one attention layer\n",
    "# attention_probs = Dense(100, activation='softmax', name='attention_probs')(input_layer)\n",
    "# attention_mul = merge([input_layer, attention_probs], output_shape=(80,), name='attention_mul', mode='mul')\n",
    "# model.add(attention_mul)\n",
    "# model.add(input_layer)\n",
    "# model.add(Dense(60, activation='relu'))\n",
    "model.add(Embedding(100, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) # recurrent\n",
    "model.add(Dense(64, activation='softmax', name='attention_probs'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and fit\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Test the model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN network with attention - does not perform well currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of sents in chat: 14.598977355734112\n",
      "Training\n",
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/40\n",
      "950/950 [==============================] - 6s - loss: 0.6687 - acc: 0.6137 - val_loss: 0.7034 - val_acc: 0.5400 loss: 0.6745 - ac\n",
      "Epoch 2/40\n",
      "950/950 [==============================] - 6s - loss: 0.6673 - acc: 0.6179 - val_loss: 0.6933 - val_acc: 0.5400\n",
      "Epoch 3/40\n",
      "950/950 [==============================] - 7s - loss: 0.6710 - acc: 0.6179 - val_loss: 0.6932 - val_acc: 0.5400\n",
      "Epoch 4/40\n",
      "950/950 [==============================] - 7s - loss: 0.6690 - acc: 0.6179 - val_loss: 0.7068 - val_acc: 0.5400\n",
      "Epoch 5/40\n",
      "950/950 [==============================] - 7s - loss: 0.6668 - acc: 0.6179 - val_loss: 0.7021 - val_acc: 0.5400\n",
      "Epoch 6/40\n",
      "950/950 [==============================] - 7s - loss: 0.6653 - acc: 0.6179 - val_loss: 0.6973 - val_acc: 0.5400\n",
      "Epoch 7/40\n",
      "950/950 [==============================] - 7s - loss: 0.6675 - acc: 0.6179 - val_loss: 0.6959 - val_acc: 0.5400\n",
      "Epoch 8/40\n",
      "950/950 [==============================] - 7s - loss: 0.6675 - acc: 0.6179 - val_loss: 0.7002 - val_acc: 0.5400\n",
      "Epoch 9/40\n",
      "950/950 [==============================] - 7s - loss: 0.6652 - acc: 0.6179 - val_loss: 0.6985 - val_acc: 0.5400\n",
      "Epoch 10/40\n",
      "950/950 [==============================] - 7s - loss: 0.6680 - acc: 0.6179 - val_loss: 0.6986 - val_acc: 0.5400\n",
      "Epoch 11/40\n",
      "950/950 [==============================] - 7s - loss: 0.6651 - acc: 0.6179 - val_loss: 0.7075 - val_acc: 0.5400\n",
      "Epoch 12/40\n",
      "950/950 [==============================] - 7s - loss: 0.6676 - acc: 0.6179 - val_loss: 0.6981 - val_acc: 0.5400\n",
      "Epoch 13/40\n",
      "950/950 [==============================] - 7s - loss: 0.6645 - acc: 0.6179 - val_loss: 0.7035 - val_acc: 0.5400\n",
      "Epoch 14/40\n",
      "950/950 [==============================] - 7s - loss: 0.6674 - acc: 0.6179 - val_loss: 0.7040 - val_acc: 0.5400\n",
      "Epoch 15/40\n",
      "950/950 [==============================] - 7s - loss: 0.6660 - acc: 0.6179 - val_loss: 0.6973 - val_acc: 0.5400\n",
      "Epoch 16/40\n",
      "950/950 [==============================] - 7s - loss: 0.6655 - acc: 0.6179 - val_loss: 0.7057 - val_acc: 0.5400\n",
      "Epoch 17/40\n",
      "950/950 [==============================] - 7s - loss: 0.6684 - acc: 0.6179 - val_loss: 0.6983 - val_acc: 0.5400\n",
      "Epoch 18/40\n",
      "950/950 [==============================] - 7s - loss: 0.6667 - acc: 0.6179 - val_loss: 0.7060 - val_acc: 0.5400\n",
      "Epoch 19/40\n",
      "950/950 [==============================] - 7s - loss: 0.6680 - acc: 0.6179 - val_loss: 0.7061 - val_acc: 0.5400\n",
      "Epoch 20/40\n",
      "950/950 [==============================] - 7s - loss: 0.6658 - acc: 0.6179 - val_loss: 0.6973 - val_acc: 0.5400\n",
      "Epoch 21/40\n",
      "950/950 [==============================] - 7s - loss: 0.6669 - acc: 0.6179 - val_loss: 0.7025 - val_acc: 0.5400\n",
      "Epoch 22/40\n",
      "950/950 [==============================] - 7s - loss: 0.6657 - acc: 0.6179 - val_loss: 0.7001 - val_acc: 0.5400\n",
      "Epoch 23/40\n",
      "950/950 [==============================] - 7s - loss: 0.6666 - acc: 0.6179 - val_loss: 0.7022 - val_acc: 0.5400\n",
      "Epoch 24/40\n",
      "950/950 [==============================] - 7s - loss: 0.6666 - acc: 0.6179 - val_loss: 0.7017 - val_acc: 0.5400\n",
      "Epoch 25/40\n",
      "950/950 [==============================] - 7s - loss: 0.6664 - acc: 0.6179 - val_loss: 0.6997 - val_acc: 0.5400\n",
      "Epoch 26/40\n",
      "950/950 [==============================] - 7s - loss: 0.6668 - acc: 0.6179 - val_loss: 0.7001 - val_acc: 0.5400\n",
      "Epoch 27/40\n",
      "950/950 [==============================] - 7s - loss: 0.6658 - acc: 0.6179 - val_loss: 0.7028 - val_acc: 0.5400\n",
      "Epoch 28/40\n",
      "950/950 [==============================] - 7s - loss: 0.6672 - acc: 0.6179 - val_loss: 0.7000 - val_acc: 0.5400\n",
      "Epoch 29/40\n",
      "950/950 [==============================] - 7s - loss: 0.6651 - acc: 0.6179 - val_loss: 0.7036 - val_acc: 0.5400\n",
      "Epoch 30/40\n",
      "950/950 [==============================] - 7s - loss: 0.6676 - acc: 0.6179 - val_loss: 0.6986 - val_acc: 0.5400\n",
      "Epoch 31/40\n",
      "950/950 [==============================] - 8s - loss: 0.6671 - acc: 0.6179 - val_loss: 0.7010 - val_acc: 0.5400\n",
      "Epoch 32/40\n",
      "950/950 [==============================] - 7s - loss: 0.6670 - acc: 0.6179 - val_loss: 0.7034 - val_acc: 0.5400\n",
      "Epoch 33/40\n",
      "950/950 [==============================] - 7s - loss: 0.6649 - acc: 0.6179 - val_loss: 0.7015 - val_acc: 0.5400\n",
      "Epoch 34/40\n",
      "950/950 [==============================] - 7s - loss: 0.6662 - acc: 0.6179 - val_loss: 0.7030 - val_acc: 0.5400\n",
      "Epoch 35/40\n",
      "950/950 [==============================] - 7s - loss: 0.6664 - acc: 0.6179 - val_loss: 0.7030 - val_acc: 0.5400\n",
      "Epoch 36/40\n",
      "950/950 [==============================] - 7s - loss: 0.6658 - acc: 0.6179 - val_loss: 0.7040 - val_acc: 0.5400\n",
      "Epoch 37/40\n",
      "950/950 [==============================] - 7s - loss: 0.6665 - acc: 0.6179 - val_loss: 0.7004 - val_acc: 0.5400\n",
      "Epoch 38/40\n",
      "950/950 [==============================] - 7s - loss: 0.6664 - acc: 0.6179 - val_loss: 0.7018 - val_acc: 0.5400\n",
      "Epoch 39/40\n",
      "950/950 [==============================] - 16s - loss: 0.6668 - acc: 0.6179 - val_loss: 0.6972 - val_acc: 0.5400\n",
      "Epoch 40/40\n",
      "950/950 [==============================] - 8s - loss: 0.6675 - acc: 0.6179 - val_loss: 0.6997 - val_acc: 0.5400\n",
      "369/369 [==============================] - 1s     \n",
      "\n",
      "Test loss / test accuracy = 0.6713 / 0.6043\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, merge, Embedding, recurrent\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import sys\n",
    "# should change to relative import\n",
    "sys.path.insert(0, '/home/amirgilad/Documents/git/NLPProj/kerasAttention')\n",
    "import attention_dense\n",
    "import attention_lstm\n",
    "\n",
    "input_dims = x_train.shape\n",
    "train_size = 1000\n",
    "\n",
    "# Build doc2Vec model for the chats\n",
    "d = build_label_dict()\n",
    "chat_model = gen_chat_model(d)\n",
    "\n",
    "# Load the data for the train and test\n",
    "# The last param is the num. of rows we take from the begining of the chat\n",
    "num_of_rows_from_start_of_chat = 6\n",
    "X, Y = infer_vectors(chat_model, d, num_of_rows_from_start_of_chat)\n",
    "\n",
    "# Divide into training and test data\n",
    "x_train, y_train = X[:train_size], Y[:train_size]\n",
    "x_test, y_test = X[train_size:], Y[train_size:]\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# # model.add(Dense(80, input_dim=100, activation='relu'))\n",
    "# input_layer = Dense(80, input_dim=100, activation='relu')\n",
    "\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 100\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "# Define the model\n",
    "sentence = Input(shape=(input_dims[1],), dtype='int32')\n",
    "encoded_sentence = Embedding(100, EMBED_HIDDEN_SIZE)(sentence)\n",
    "encoded_sentence = layers.Dropout(0.3)(encoded_sentence)\n",
    "attention_probs = Dense(100, activation='softmax', name='attention_probs')(encoded_sentence)# one attention layer\n",
    "\n",
    "\n",
    "merged = layers.add([encoded_sentence, attention_probs])\n",
    "merged = RNN(EMBED_HIDDEN_SIZE)(merged)\n",
    "merged = layers.Dropout(0.3)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model([sentence], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training')\n",
    "model.fit([x_train], y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "loss, acc = model.evaluate([x_test], y_test,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "\n",
    "print('\\nTest loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
