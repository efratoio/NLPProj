{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple word2vec model with 200 features for each utterance, not considering other features (speaker etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec model for all chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gensim\n",
    "\n",
    "# path = r\"C:\\Users\\user\\git\\NLPProj\\Frames-dataset\\chats\"\n",
    "\n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\"\\t\")\n",
    "        d[k] = v.strip()\n",
    "    return d\n",
    "\n",
    "def gen_chat_model(label_dict):\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chats = []\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            for turn in chat['turns']:\n",
    "                sents.extend( turn['text'].split() )\n",
    "        tagged_chat = gensim.models.doc2vec.TaggedDocument(sents, label_dict[filename])\n",
    "        chats.append( tagged_chat )\n",
    "    return gensim.models.Doc2Vec(chats, size=100, window=8, min_count=1, workers=4)\n",
    "\n",
    "# d = build_label_dict()\n",
    "# chat_model = gen_chat_model(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input vectors from first parts of chats and labels from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_vectors(chat_model, label_dict, num_of_sents_from_start):\n",
    "    '''infer vectors for every sentence in the chats. \n",
    "    Will be used to load the training data and tags'''\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chats_vecs = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            lim = min(num_of_sents_from_start, len(chat['turns']))\n",
    "            for i in range( lim ):\n",
    "                sents.extend( chat['turns'][i]['text'].split() )\n",
    "#             for turn in chat['turns']:\n",
    "#                 sents.extend( turn['text'].split() )\n",
    "        sent_vec = chat_model.infer_vector( sents )\n",
    "        chats_vecs.append( sent_vec )\n",
    "        labels.append(label_dict[filename])\n",
    "    return np.array(chats_vecs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RNN - TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6848 - acc: 0.5690     \n",
      "Epoch 2/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6673 - acc: 0.6140     \n",
      "Epoch 3/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6633 - acc: 0.6140     \n",
      "Epoch 4/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6542 - acc: 0.6140     \n",
      "Epoch 5/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6400 - acc: 0.6420     \n",
      "Epoch 6/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6286 - acc: 0.6790     \n",
      "Epoch 7/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6155 - acc: 0.6860     \n",
      "Epoch 8/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6050 - acc: 0.7100     \n",
      "Epoch 9/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5917 - acc: 0.7220     \n",
      "Epoch 10/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5762 - acc: 0.7440     \n",
      "Epoch 11/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5639 - acc: 0.7530     \n",
      "Epoch 12/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5538 - acc: 0.7640     \n",
      "Epoch 13/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5392 - acc: 0.7690     \n",
      "Epoch 14/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5313 - acc: 0.7750     \n",
      "Epoch 15/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5180 - acc: 0.7910     \n",
      "Epoch 16/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4999 - acc: 0.8090     \n",
      "Epoch 17/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4915 - acc: 0.8110     \n",
      "Epoch 18/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4741 - acc: 0.8290     \n",
      "Epoch 19/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4681 - acc: 0.8320     \n",
      "Epoch 20/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4490 - acc: 0.8450     \n",
      "Epoch 21/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4365 - acc: 0.8530     \n",
      "Epoch 22/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4341 - acc: 0.8500     \n",
      "Epoch 23/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4161 - acc: 0.8620     \n",
      "Epoch 24/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4068 - acc: 0.8670     - ETA: 0s - loss: 0.3302 - acc:\n",
      "Epoch 25/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3965 - acc: 0.8770     \n",
      "Epoch 26/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3901 - acc: 0.8780     \n",
      "Epoch 27/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3787 - acc: 0.8830     \n",
      "Epoch 28/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3806 - acc: 0.8790     \n",
      "Epoch 29/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3630 - acc: 0.8890     \n",
      "Epoch 30/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3553 - acc: 0.8950     \n",
      "Epoch 31/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3478 - acc: 0.8970     \n",
      "Epoch 32/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3446 - acc: 0.8970     \n",
      "Epoch 33/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3431 - acc: 0.8960     \n",
      "Epoch 34/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3380 - acc: 0.8970     \n",
      "Epoch 35/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3323 - acc: 0.8990     \n",
      "Epoch 36/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3470 - acc: 0.8830     \n",
      "Epoch 37/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3304 - acc: 0.8990     \n",
      "Epoch 38/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3241 - acc: 0.9030     \n",
      "Epoch 39/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3351 - acc: 0.8910     \n",
      "Epoch 40/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3160 - acc: 0.9040     \n",
      "Epoch 41/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3103 - acc: 0.9060     \n",
      "Epoch 42/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3070 - acc: 0.9070     \n",
      "Epoch 43/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2993 - acc: 0.9070     \n",
      "Epoch 44/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2913 - acc: 0.9150     \n",
      "Epoch 45/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2915 - acc: 0.9130     \n",
      "Epoch 46/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2865 - acc: 0.9160     \n",
      "Epoch 47/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2814 - acc: 0.9170     \n",
      "Epoch 48/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2797 - acc: 0.9170     \n",
      "Epoch 49/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2777 - acc: 0.9180     \n",
      "Epoch 50/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2836 - acc: 0.9160     \n",
      "Epoch 51/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2845 - acc: 0.9160     \n",
      "Epoch 52/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2859 - acc: 0.9120     \n",
      "Epoch 53/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2840 - acc: 0.9140     \n",
      "Epoch 54/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2714 - acc: 0.9200     \n",
      "Epoch 55/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2669 - acc: 0.9210     \n",
      "Epoch 56/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2658 - acc: 0.9210     \n",
      "Epoch 57/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2613 - acc: 0.9230     \n",
      "Epoch 58/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2606 - acc: 0.9230     \n",
      "Epoch 59/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2600 - acc: 0.9230     \n",
      "Epoch 60/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2596 - acc: 0.9230     \n",
      "Epoch 61/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2591 - acc: 0.9230     \n",
      "Epoch 62/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2587 - acc: 0.9230     \n",
      "Epoch 63/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2584 - acc: 0.9230     \n",
      "Epoch 64/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2580 - acc: 0.9230     \n",
      "Epoch 65/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2579 - acc: 0.9230     \n",
      "Epoch 66/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2575 - acc: 0.9230     \n",
      "Epoch 67/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2572 - acc: 0.9230     \n",
      "Epoch 68/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2570 - acc: 0.9230     \n",
      "Epoch 69/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2568 - acc: 0.9230     \n",
      "Epoch 70/150\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.2621 - acc: 0.919 - 0s - loss: 0.2566 - acc: 0.9230     \n",
      "Epoch 71/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2564 - acc: 0.9230     \n",
      "Epoch 72/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2562 - acc: 0.9230     \n",
      "Epoch 73/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2561 - acc: 0.9230     \n",
      "Epoch 74/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2559 - acc: 0.9230     \n",
      "Epoch 75/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2558 - acc: 0.9230     \n",
      "Epoch 76/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2557 - acc: 0.9230     \n",
      "Epoch 77/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2555 - acc: 0.9230     \n",
      "Epoch 78/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2554 - acc: 0.9230     \n",
      "Epoch 79/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2553 - acc: 0.9230     \n",
      "Epoch 80/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2553 - acc: 0.9230     \n",
      "Epoch 81/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2551 - acc: 0.9230     \n",
      "Epoch 82/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2550 - acc: 0.9230     \n",
      "Epoch 83/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2549 - acc: 0.9230     \n",
      "Epoch 84/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2549 - acc: 0.9230     \n",
      "Epoch 85/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2548 - acc: 0.9230     \n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s - loss: 0.2548 - acc: 0.9230     - ETA: 0s - loss: 0.2131 - acc: 0.\n",
      "Epoch 87/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2547 - acc: 0.9230     \n",
      "Epoch 88/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2546 - acc: 0.9230     \n",
      "Epoch 89/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2546 - acc: 0.9230     \n",
      "Epoch 90/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2545 - acc: 0.9230     \n",
      "Epoch 91/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2544 - acc: 0.9230     \n",
      "Epoch 92/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2544 - acc: 0.9230     \n",
      "Epoch 93/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2544 - acc: 0.9230     \n",
      "Epoch 94/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2543 - acc: 0.9230     \n",
      "Epoch 95/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2543 - acc: 0.9230     \n",
      "Epoch 96/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2543 - acc: 0.9230     \n",
      "Epoch 97/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2543 - acc: 0.9230     \n",
      "Epoch 98/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3102 - acc: 0.8970     \n",
      "Epoch 99/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.8614 - acc: 0.6160     \n",
      "Epoch 100/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6921 - acc: 0.6790     \n",
      "Epoch 101/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5301 - acc: 0.7790     \n",
      "Epoch 102/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3393 - acc: 0.8880     \n",
      "Epoch 103/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2928 - acc: 0.9130     \n",
      "Epoch 104/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2738 - acc: 0.9180     \n",
      "Epoch 105/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2638 - acc: 0.9240     \n",
      "Epoch 106/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2603 - acc: 0.9240     \n",
      "Epoch 107/150\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.2464 - acc: 0.927 - 0s - loss: 0.2580 - acc: 0.9240     \n",
      "Epoch 108/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2567 - acc: 0.9240     \n",
      "Epoch 109/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2556 - acc: 0.9240     \n",
      "Epoch 110/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2548 - acc: 0.9240     \n",
      "Epoch 111/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2543 - acc: 0.9240     \n",
      "Epoch 112/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2538 - acc: 0.9240     \n",
      "Epoch 113/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2534 - acc: 0.9240     \n",
      "Epoch 114/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2531 - acc: 0.9240     \n",
      "Epoch 115/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2529 - acc: 0.9240     \n",
      "Epoch 116/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2527 - acc: 0.9240     \n",
      "Epoch 117/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2526 - acc: 0.9240     \n",
      "Epoch 118/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2524 - acc: 0.9240     \n",
      "Epoch 119/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2524 - acc: 0.9240     \n",
      "Epoch 120/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2523 - acc: 0.9240     \n",
      "Epoch 121/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2522 - acc: 0.9240     \n",
      "Epoch 122/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2522 - acc: 0.9240     \n",
      "Epoch 123/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2521 - acc: 0.9240     \n",
      "Epoch 124/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2521 - acc: 0.9240     \n",
      "Epoch 125/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2520 - acc: 0.9240     \n",
      "Epoch 126/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2520 - acc: 0.9240     \n",
      "Epoch 127/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2520 - acc: 0.9240     \n",
      "Epoch 128/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2519 - acc: 0.9240     \n",
      "Epoch 129/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2519 - acc: 0.9240     \n",
      "Epoch 130/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2519 - acc: 0.9240     \n",
      "Epoch 131/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2518 - acc: 0.9240     \n",
      "Epoch 132/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2519 - acc: 0.9240     \n",
      "Epoch 133/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2518 - acc: 0.9240     \n",
      "Epoch 134/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2518 - acc: 0.9240     \n",
      "Epoch 135/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2518 - acc: 0.9240     - ETA: 0s - loss: 0.1914 - acc:\n",
      "Epoch 136/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2518 - acc: 0.9240     \n",
      "Epoch 137/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2518 - acc: 0.9240     \n",
      "Epoch 138/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2518 - acc: 0.9240     \n",
      "Epoch 139/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 140/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 141/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 142/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 143/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 144/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 145/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2516 - acc: 0.9240     \n",
      "Epoch 146/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 147/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2516 - acc: 0.9240     \n",
      "Epoch 148/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 149/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2517 - acc: 0.9240     \n",
      "Epoch 150/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2516 - acc: 0.9240     \n",
      "  32/1369 [..............................] - ETA: 0s\n",
      "acc: 84.22%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, merge\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import sys\n",
    "# should change to relative import\n",
    "sys.path.insert(0, '/home/amirgilad/Documents/git/NLPProj/kerasAttention')\n",
    "import attention_dense\n",
    "import attention_lstm\n",
    "\n",
    "input_dims = x_train.shape\n",
    "train_size = 1000\n",
    "\n",
    "# Build doc2Vec model for the chats\n",
    "d = build_label_dict()\n",
    "chat_model = gen_chat_model(d)\n",
    "\n",
    "# Load the data for the train and test\n",
    "# The last param is the num. of rows we take from the begining of the chat\n",
    "num_of_rows_from_start_of_chat = 3\n",
    "X, Y = infer_vectors(chat_model, d, num_of_rows_from_start_of_chat)\n",
    "\n",
    "# Divide into training and test data\n",
    "x_train, y_train = X[:train_size], Y[:train_size]\n",
    "x_test, y_test = X[train_size:], Y[train_size:]\n",
    "\n",
    "# one attention layer\n",
    "# inputs = Input(shape=(input_dims))#(input_dims,)\n",
    "# attention_probs = Dense(input_dims[1], activation='softmax', name='attention_probs')(inputs)\n",
    "# attention_mul = merge([inputs, attention_probs], output_shape=(1,), name='attention_mul', mode='mul')\n",
    "\n",
    "# define the model\n",
    "# model = Model(input=inputs, output=attention_mul)\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(80, input_dim=100, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(40, activation='softmax', name='attention_probs'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and fit\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=150, batch_size=10)\n",
    "\n",
    "# Test the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
