{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple word2vec model with 200 features for each utterance, not considering other features (speaker etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec model for all chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gensim\n",
    "\n",
    "# path = r\"C:\\Users\\user\\git\\NLPProj\\Frames-dataset\\chats\"\n",
    "\n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\"\\t\")\n",
    "        d[k] = v.strip()\n",
    "    return d\n",
    "\n",
    "def gen_chat_model(label_dict):\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chats = []\n",
    "    avg_chat_len = 0\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            for turn in chat['turns']:\n",
    "                sents.extend( turn['text'].split() )\n",
    "                avg_chat_len += 1\n",
    "        tagged_chat = gensim.models.doc2vec.TaggedDocument(sents, label_dict[filename])\n",
    "        chats.append( tagged_chat )\n",
    "    avg_chat_len /= len(chats)\n",
    "    print(\"average number of sents in chat: \" + str(avg_chat_len))\n",
    "    return gensim.models.Doc2Vec(chats, size=100, window=8, min_count=1, workers=4)\n",
    "\n",
    "# d = build_label_dict()\n",
    "# chat_model = gen_chat_model(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input vectors from first parts of chats and labels from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_vectors(chat_model, label_dict, num_of_sents_from_start):\n",
    "    '''infer vectors for every sentence in the chats. \n",
    "    Will be used to load the training data and tags'''\n",
    "    chat_path = r\"/home/amirgilad/Documents/git/NLPProj/Frames-dataset/chats\"\n",
    "    chats_vecs = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        sents = [] # chat level docs\n",
    "        if 'turns' in chat:\n",
    "            lim = min(num_of_sents_from_start, len(chat['turns']))\n",
    "            for i in range( lim ):\n",
    "                sents.extend( chat['turns'][i]['text'].split() )\n",
    "#             for turn in chat['turns']:\n",
    "#                 sents.extend( turn['text'].split() )\n",
    "        sent_vec = chat_model.infer_vector( sents )\n",
    "        chats_vecs.append( sent_vec )\n",
    "        labels.append(label_dict[filename])\n",
    "    return np.array(chats_vecs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the RNN - TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of sents in chat: 14.598977355734112\n",
      "Epoch 1/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6819 - acc: 0.6140     \n",
      "Epoch 2/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6639 - acc: 0.6140     \n",
      "Epoch 3/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6501 - acc: 0.6140     \n",
      "Epoch 4/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6320 - acc: 0.6650     \n",
      "Epoch 5/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6143 - acc: 0.7050     \n",
      "Epoch 6/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.6002 - acc: 0.7160     \n",
      "Epoch 7/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5849 - acc: 0.7410     \n",
      "Epoch 8/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5727 - acc: 0.7490     \n",
      "Epoch 9/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5603 - acc: 0.7550     \n",
      "Epoch 10/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5465 - acc: 0.7700     \n",
      "Epoch 11/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5315 - acc: 0.7810     \n",
      "Epoch 12/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5220 - acc: 0.7870     \n",
      "Epoch 13/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.5057 - acc: 0.7950     \n",
      "Epoch 14/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4895 - acc: 0.8200     \n",
      "Epoch 15/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4680 - acc: 0.8350     \n",
      "Epoch 16/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4510 - acc: 0.8460     \n",
      "Epoch 17/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4352 - acc: 0.8590     \n",
      "Epoch 18/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4307 - acc: 0.8560     \n",
      "Epoch 19/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.4089 - acc: 0.8760     \n",
      "Epoch 20/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3955 - acc: 0.8830     \n",
      "Epoch 21/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3952 - acc: 0.8690     \n",
      "Epoch 22/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3596 - acc: 0.8990     \n",
      "Epoch 23/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3429 - acc: 0.9140     \n",
      "Epoch 24/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3312 - acc: 0.9150     \n",
      "Epoch 25/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3189 - acc: 0.9180     \n",
      "Epoch 26/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3090 - acc: 0.9220     \n",
      "Epoch 27/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3045 - acc: 0.9180     \n",
      "Epoch 28/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2902 - acc: 0.9290     \n",
      "Epoch 29/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2813 - acc: 0.9330     \n",
      "Epoch 30/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2634 - acc: 0.9400     \n",
      "Epoch 31/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2567 - acc: 0.9400     \n",
      "Epoch 32/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2499 - acc: 0.9430     \n",
      "Epoch 33/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2461 - acc: 0.9430     \n",
      "Epoch 34/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2472 - acc: 0.9420     \n",
      "Epoch 35/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2344 - acc: 0.9470     \n",
      "Epoch 36/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2278 - acc: 0.9480     \n",
      "Epoch 37/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2237 - acc: 0.9480     \n",
      "Epoch 38/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2199 - acc: 0.9490     \n",
      "Epoch 39/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2146 - acc: 0.9510     \n",
      "Epoch 40/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2115 - acc: 0.9510     \n",
      "Epoch 41/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2092 - acc: 0.9510     \n",
      "Epoch 42/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2076 - acc: 0.9510     \n",
      "Epoch 43/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2222 - acc: 0.9410     \n",
      "Epoch 44/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2301 - acc: 0.9380     \n",
      "Epoch 45/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2330 - acc: 0.9370     \n",
      "Epoch 46/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2074 - acc: 0.9490     \n",
      "Epoch 47/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1957 - acc: 0.9550     \n",
      "Epoch 48/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1916 - acc: 0.9550     \n",
      "Epoch 49/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1904 - acc: 0.9550     \n",
      "Epoch 50/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1893 - acc: 0.9550     \n",
      "Epoch 51/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1886 - acc: 0.9550     \n",
      "Epoch 52/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1880 - acc: 0.9550     \n",
      "Epoch 53/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1855 - acc: 0.9560     \n",
      "Epoch 54/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1842 - acc: 0.9560     \n",
      "Epoch 55/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1835 - acc: 0.9560     \n",
      "Epoch 56/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1829 - acc: 0.9560     \n",
      "Epoch 57/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1825 - acc: 0.9560     \n",
      "Epoch 58/150\n",
      "1000/1000 [==============================] - 2s - loss: 0.1820 - acc: 0.9560     \n",
      "Epoch 59/150\n",
      "1000/1000 [==============================] - 2s - loss: 0.1816 - acc: 0.9560     - ETA: 0s - loss: 0.1822 -\n",
      "Epoch 60/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1812 - acc: 0.9560     \n",
      "Epoch 61/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1809 - acc: 0.9560     \n",
      "Epoch 62/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1806 - acc: 0.9560     \n",
      "Epoch 63/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1803 - acc: 0.9560     \n",
      "Epoch 64/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1801 - acc: 0.9560     \n",
      "Epoch 65/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1798 - acc: 0.9560     \n",
      "Epoch 66/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1796 - acc: 0.9560     \n",
      "Epoch 67/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1794 - acc: 0.9560     \n",
      "Epoch 68/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1793 - acc: 0.9560     \n",
      "Epoch 69/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1791 - acc: 0.9560     \n",
      "Epoch 70/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1827 - acc: 0.9540     \n",
      "Epoch 71/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.3473 - acc: 0.8810     \n",
      "Epoch 72/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2277 - acc: 0.9360     \n",
      "Epoch 73/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2077 - acc: 0.9400     \n",
      "Epoch 74/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2005 - acc: 0.9440     \n",
      "Epoch 75/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1843 - acc: 0.9550     \n",
      "Epoch 76/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1794 - acc: 0.9560     \n",
      "Epoch 77/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1764 - acc: 0.9570     \n",
      "Epoch 78/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1759 - acc: 0.9570     \n",
      "Epoch 79/150\n",
      "1000/1000 [==============================] - 1s - loss: 0.1756 - acc: 0.9570     \n",
      "Epoch 80/150\n",
      "1000/1000 [==============================] - 2s - loss: 0.1753 - acc: 0.9570     - ETA: 1s - loss: 0.1735 - acc: - ETA: 0s - loss: 0.1848 \n",
      "Epoch 81/150\n",
      "1000/1000 [==============================] - 1s - loss: 0.1752 - acc: 0.9570     \n",
      "Epoch 82/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1749 - acc: 0.9570     \n",
      "Epoch 83/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1747 - acc: 0.9570     \n",
      "Epoch 84/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1716 - acc: 0.9580     \n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s - loss: 0.1713 - acc: 0.9580     \n",
      "Epoch 86/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1711 - acc: 0.9580     \n",
      "Epoch 87/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1710 - acc: 0.9580     \n",
      "Epoch 88/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1708 - acc: 0.9580     \n",
      "Epoch 89/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1708 - acc: 0.9580     \n",
      "Epoch 90/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1706 - acc: 0.9580     \n",
      "Epoch 91/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1705 - acc: 0.9580     \n",
      "Epoch 92/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1704 - acc: 0.9580     \n",
      "Epoch 93/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1703 - acc: 0.9580     \n",
      "Epoch 94/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1671 - acc: 0.9590     \n",
      "Epoch 95/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1666 - acc: 0.9590     \n",
      "Epoch 96/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1663 - acc: 0.9590     \n",
      "Epoch 97/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1661 - acc: 0.9590     \n",
      "Epoch 98/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1645 - acc: 0.9600     \n",
      "Epoch 99/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1632 - acc: 0.9600     \n",
      "Epoch 100/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1624 - acc: 0.9600     \n",
      "Epoch 101/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1621 - acc: 0.9600     \n",
      "Epoch 102/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1690 - acc: 0.9580     \n",
      "Epoch 103/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2125 - acc: 0.9400     \n",
      "Epoch 104/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.2204 - acc: 0.9320     \n",
      "Epoch 105/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1648 - acc: 0.9590     \n",
      "Epoch 106/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1818 - acc: 0.9470     \n",
      "Epoch 107/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1597 - acc: 0.9600     \n",
      "Epoch 108/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1542 - acc: 0.9620     \n",
      "Epoch 109/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1501 - acc: 0.9630     \n",
      "Epoch 110/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1490 - acc: 0.9640     \n",
      "Epoch 111/150\n",
      "1000/1000 [==============================] - 2s - loss: 0.1485 - acc: 0.9630     \n",
      "Epoch 112/150\n",
      "1000/1000 [==============================] - 2s - loss: 0.1472 - acc: 0.9640     - ETA: 3s - l\n",
      "Epoch 113/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1468 - acc: 0.9640     \n",
      "Epoch 114/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1466 - acc: 0.9640     \n",
      "Epoch 115/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1463 - acc: 0.9640     \n",
      "Epoch 116/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1462 - acc: 0.9640     \n",
      "Epoch 117/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1459 - acc: 0.9640     \n",
      "Epoch 118/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1458 - acc: 0.9640     \n",
      "Epoch 119/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1456 - acc: 0.9640     \n",
      "Epoch 120/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1454 - acc: 0.9640     \n",
      "Epoch 121/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1451 - acc: 0.9640     \n",
      "Epoch 122/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1449 - acc: 0.9640     \n",
      "Epoch 123/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1446 - acc: 0.9640     \n",
      "Epoch 124/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1442 - acc: 0.9640     \n",
      "Epoch 125/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1440 - acc: 0.9640     \n",
      "Epoch 126/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1433 - acc: 0.9640     \n",
      "Epoch 127/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1428 - acc: 0.9640     \n",
      "Epoch 128/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1423 - acc: 0.9640     \n",
      "Epoch 129/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1419 - acc: 0.9640     \n",
      "Epoch 130/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1414 - acc: 0.9640     \n",
      "Epoch 131/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1411 - acc: 0.9640     \n",
      "Epoch 132/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1408 - acc: 0.9640     \n",
      "Epoch 133/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1406 - acc: 0.9640     \n",
      "Epoch 134/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1404 - acc: 0.9640     \n",
      "Epoch 135/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1401 - acc: 0.9650     \n",
      "Epoch 136/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1397 - acc: 0.9650     \n",
      "Epoch 137/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1394 - acc: 0.9650     \n",
      "Epoch 138/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1392 - acc: 0.9650     \n",
      "Epoch 139/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1390 - acc: 0.9650     \n",
      "Epoch 140/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1389 - acc: 0.9650     \n",
      "Epoch 141/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1388 - acc: 0.9650     \n",
      "Epoch 142/150\n",
      "1000/1000 [==============================] - 2s - loss: 0.1387 - acc: 0.9650     - ETA: 2s - \n",
      "Epoch 143/150\n",
      "1000/1000 [==============================] - 1s - loss: 0.1386 - acc: 0.9650     \n",
      "Epoch 144/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1385 - acc: 0.9650     \n",
      "Epoch 145/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1385 - acc: 0.9650     \n",
      "Epoch 146/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1384 - acc: 0.9650     \n",
      "Epoch 147/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1383 - acc: 0.9650     \n",
      "Epoch 148/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1382 - acc: 0.9650     \n",
      "Epoch 149/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1381 - acc: 0.9650     \n",
      "Epoch 150/150\n",
      "1000/1000 [==============================] - 0s - loss: 0.1380 - acc: 0.9650     \n",
      " 32/369 [=>............................] - ETA: 0s\n",
      "acc: 64.77%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, merge\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import sys\n",
    "# should change to relative import\n",
    "sys.path.insert(0, '/home/amirgilad/Documents/git/NLPProj/kerasAttention')\n",
    "import attention_dense\n",
    "import attention_lstm\n",
    "\n",
    "input_dims = x_train.shape\n",
    "train_size = 1000\n",
    "\n",
    "# Build doc2Vec model for the chats\n",
    "d = build_label_dict()\n",
    "chat_model = gen_chat_model(d)\n",
    "\n",
    "# Load the data for the train and test\n",
    "# The last param is the num. of rows we take from the begining of the chat\n",
    "num_of_rows_from_start_of_chat = 5\n",
    "X, Y = infer_vectors(chat_model, d, num_of_rows_from_start_of_chat)\n",
    "\n",
    "# Divide into training and test data\n",
    "x_train, y_train = X[:train_size], Y[:train_size]\n",
    "x_test, y_test = X[train_size:], Y[train_size:]\n",
    "\n",
    "# one attention layer\n",
    "# inputs = Input(shape=(input_dims))#(input_dims,)\n",
    "# attention_probs = Dense(input_dims[1], activation='softmax', name='attention_probs')(inputs)\n",
    "# attention_mul = merge([inputs, attention_probs], output_shape=(1,), name='attention_mul', mode='mul')\n",
    "\n",
    "# define the model\n",
    "# model = Model(input=inputs, output=attention_mul)\n",
    "# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(80, input_dim=100, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(40, activation='softmax', name='attention_probs'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and fit\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=150, batch_size=10)\n",
    "\n",
    "# Test the model\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
