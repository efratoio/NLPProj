{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, re, json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import time\n",
    "from numpy import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def invert_dict(d):\n",
    "    return {v:k for k,v in d.iteritems()}\n",
    "\n",
    "def flatten1(lst):\n",
    "    return list(itertools.chain.from_iterable(lst))\n",
    "\n",
    "def load_wv_pandas(fname):\n",
    "    return pd.read_hdf(fname, 'data')\n",
    "\n",
    "def extract_wv(df):\n",
    "    num_to_word = dict(enumerate(df.index))\n",
    "    word_to_num = invert_dict(num_to_word)\n",
    "    wv = df.as_matrix()\n",
    "    return wv, word_to_num, num_to_word\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"UUUNKKK\" # unknown token\n",
    "\n",
    "\n",
    "##\n",
    "# Utility functions used to create dataset\n",
    "##\n",
    "def augment_wv(df, extra=[\"UUUNKKK\"]):\n",
    "    for e in extra:\n",
    "        df.loc[e] = zeros(len(df.columns))\n",
    "\n",
    "def prune_wv(df, vocab, extra=[\"UUUNKKK\"]):\n",
    "    \"\"\"Prune word vectors to vocabulary.\"\"\"\n",
    "    items = set(vocab).union(set(extra))\n",
    "    return df.filter(items=items, axis='index')\n",
    "\n",
    "def load_wv_raw(fname):\n",
    "    return pd.read_table(fname, sep=\"\\s+\",\n",
    "                         header=None,\n",
    "                         index_col=0,\n",
    "                         quoting=3)\n",
    "\n",
    "def load_dataset(fname):\n",
    "    docs = []\n",
    "    with open(fname) as fd:\n",
    "        cur = []\n",
    "        for line in fd:\n",
    "            # new sentence on -DOCSTART- or blank line\n",
    "            if re.match(r\"-DOCSTART-.+\", line) or (len(line.strip()) == 0):\n",
    "                if len(cur) > 0:\n",
    "                    docs.append(cur)\n",
    "                cur = []\n",
    "            else: # read in tokens\n",
    "                cur.append(line.strip().split(\"\\t\",1))\n",
    "        # flush running buffer\n",
    "        docs.append(cur)\n",
    "    return docs\n",
    "\n",
    "def extract_tag_set(docs):\n",
    "    tags = set(flatten1([[t[1].split(\"|\")[0] for t in d] for d in docs]))\n",
    "    return tags\n",
    "\n",
    "def extract_word_set(docs):\n",
    "    words = set(flatten1([[t[0] for t in d] for d in docs]))\n",
    "    return words\n",
    "\n",
    "def pad_sequence(seq, left=1, right=1):\n",
    "    return left*[(\"<s>\", \"\")] + seq + right*[(\"</s>\", \"\")]\n",
    "\n",
    "##\n",
    "# For window models\n",
    "def seq_to_windows(words, tags, word_to_num, tag_to_num, left=1, right=1):\n",
    "    ns = len(words)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(ns):\n",
    "        if words[i] == \"<s>\" or words[i] == \"</s>\":\n",
    "            continue # skip sentence delimiters\n",
    "        tagn = tag_to_num[tags[i]]\n",
    "        idxs = [word_to_num[words[ii]]\n",
    "                for ii in range(i - left, i + right + 1)]\n",
    "        X.append(idxs)\n",
    "        y.append(tagn)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def docs_to_windows(docs, word_to_num, tag_to_num, wsize=3):\n",
    "    pad = (wsize - 1)/2\n",
    "    docs = flatten1([pad_sequence(seq, left=pad, right=pad) for seq in docs])\n",
    "\n",
    "    words, tags = zip(*docs)\n",
    "    words = [canonicalize_word(w, word_to_num) for w in words]\n",
    "    tags = [t.split(\"|\")[0] for t in tags]\n",
    "    return seq_to_windows(words, tags, word_to_num, tag_to_num, pad, pad)\n",
    "\n",
    "def window_to_vec(window, L):\n",
    "    \"\"\"Concatenate word vectors for a given window.\"\"\"\n",
    "    return concatenate([L[i] for i in window])\n",
    "\n",
    "##\n",
    "# For fixed-window LM:\n",
    "# each row of X is a list of word indices\n",
    "# each entry of y is the word index to predict\n",
    "def seq_to_lm_windows(words, word_to_num, ngram=2):\n",
    "    ns = len(words)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(ns):\n",
    "        if words[i] == \"<s>\":\n",
    "            continue # skip sentence begin, but do predict end\n",
    "        idxs = [word_to_num[words[ii]]\n",
    "                for ii in range(i - ngram + 1, i + 1)]\n",
    "        X.append(idxs[:-1])\n",
    "        y.append(idxs[-1])\n",
    "    return array(X), array(y)\n",
    "\n",
    "def docs_to_lm_windows(docs, word_to_num, ngram=2):\n",
    "    docs = flatten1([pad_sequence(seq, left=(ngram-1), right=1)\n",
    "                     for seq in docs])\n",
    "    words = [canonicalize_word(wt[0], word_to_num) for wt in docs]\n",
    "    return seq_to_lm_windows(words, word_to_num, ngram)\n",
    "\n",
    "\n",
    "##\n",
    "# For RNN LM\n",
    "# just convert each sentence to a list of indices\n",
    "# after padding each with <s> ... </s> tokens\n",
    "def seq_to_indices(words, word_to_num):\n",
    "    return array([word_to_num[w] for w in words])\n",
    "\n",
    "def docs_to_indices(docs, word_to_num):\n",
    "    docs = [pad_sequence(seq, left=2, right=1) for seq in docs]\n",
    "    ret = []\n",
    "    for seq in docs:\n",
    "        words = [canonicalize_word(wt[0], word_to_num) for wt in seq]\n",
    "        ret.append(seq_to_indices(words, word_to_num))\n",
    "\n",
    "    # return as numpy array for fancier slicing\n",
    "    return array(ret, dtype=object)\n",
    "\n",
    "def offset_seq(seq):\n",
    "    return seq[:-1], seq[1:]\n",
    "\n",
    "def seqs_to_lmXY(seqs):\n",
    "    X, Y = zip(*[offset_seq(s) for s in seqs])\n",
    "    return array(X, dtype=object), array(Y, dtype=object)\n",
    "\n",
    "##\n",
    "# For RNN tagger\n",
    "# return X, Y as lists\n",
    "# where X[i] is indices, Y[i] is tags for a sequence\n",
    "# NOTE: this does not use padding tokens!\n",
    "#    (RNN should natively handle begin/end)\n",
    "def docs_to_tag_sequence(docs, word_to_num, tag_to_num):\n",
    "    # docs = [pad_sequence(seq, left=1, right=1) for seq in docs]\n",
    "    X = []\n",
    "    Y = []\n",
    "    for seq in docs:\n",
    "        if len(seq) < 1: continue\n",
    "        words, tags = zip(*seq)\n",
    "\n",
    "        words = [canonicalize_word(w, word_to_num) for w in words]\n",
    "        x = seq_to_indices(words, word_to_num)\n",
    "        X.append(x)\n",
    "\n",
    "        tags = [t.split(\"|\")[0] for t in tags]\n",
    "        y = seq_to_indices(tags, tag_to_num)\n",
    "        Y.append(y)\n",
    "\n",
    "    # return as numpy array for fancier slicing\n",
    "    return array(X, dtype=object), array(Y, dtype=object)\n",
    "\n",
    "def idxs_to_matrix(idxs, L):\n",
    "    \"\"\"Return a matrix X with each row\n",
    "    as a word vector for the corresponding\n",
    "    index in idxs.\"\"\"\n",
    "    return vstack([L[i] for i in idxs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
