{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#UTF-8\n",
    "import json\n",
    "import pickle\n",
    "from os import path\n",
    "import os,re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "dict_file_path = path.join(\"..\",\"frames-data-v1\",\"Frames-dataset\",\"word_dict\")\n",
    "frames_file_path = path.join(\"..\",\"frames-data-v1\",\"Frames-dataset\",\"frames.json\")\n",
    "chat_file_path = path.join(\"..\",\"frames-data-v1\",\"Frames-dataset\",\"chats_dict\")\n",
    "chatvec_file_path = path.join(\"..\",\"frames-data-v1\",\"Frames-dataset\",\"chats_vecs\")\n",
    "chat_text_file_path = path.join(\"..\",\"frames-data-v1\",\"Frames-dataset\",\"chats_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"UUUNKKK\" # unknown token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_dicts():\n",
    "    chats = []\n",
    "    with open(frames_file_path,'r') as f:\n",
    "        chats = json.load(f)\n",
    "\n",
    "    word_dict = {}    \n",
    "    for chat in chats:\n",
    "        chat_word_dict={}\n",
    "        for turn in chat[\"turns\"]:\n",
    "            for word in turn[\"text\"]:\n",
    "                word = canonicalize_word(word)\n",
    "                if word in word_dict:\n",
    "                    word_dict[word]+=1\n",
    "                else:\n",
    "                    word_dict[word]=1\n",
    "\n",
    "                if word in chat_word_dict:\n",
    "                    chat_word_dict[word]+=1\n",
    "                else:\n",
    "                    chat_word_dict[word]=1\n",
    "        with open(path.join(chat_file_path,chat[\"id\"]+\".dict\"),\"w\") as f:\n",
    "            pickle.dump(chat_word_dict,f)\n",
    "    \n",
    "    with open(dict_file_path,\"w\") as f:\n",
    "        pickle.dump(word_dict,f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vectors():\n",
    "    with open(dict_file_path,'r') as f:\n",
    "        word_dict  = pickle.load(f)\n",
    "    lst = []\n",
    "    for key in word_dict:\n",
    "        lst.append(key)\n",
    "    \n",
    "    for file_name in os.listdir(chat_file_path):\n",
    "        vec = []\n",
    "        ind = file_name.split('.')[0]\n",
    "        chat_dict={}\n",
    "        with open(path.join(chat_file_path,file_name),'r') as f:\n",
    "            chat_dict = pickle.load(f)\n",
    "        for word in lst:\n",
    "            if word in chat_dict:\n",
    "                vec.append(chat_dict[word])\n",
    "            else:\n",
    "                vec.append(0)\n",
    "        vec = np.array(vec)\n",
    "        with open(path.join(chatvec_file_path,ind),'w') as f:\n",
    "            pickle.dump(vec,f)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_text(prnct):\n",
    "    global chat_text_file_path\n",
    "    chat_text_file_path = path.join(\"..\",\"frames-data-v1\",\"Frames-dataset\",\"chats_text\")\n",
    "    chats = []\n",
    "    with open(frames_file_path,'r') as f:\n",
    "        chats = json.load(f)\n",
    "\n",
    "    for chat in chats:\n",
    "        text=[]\n",
    "        booked=\"False\"\n",
    "        for turn in chat[\"turns\"]:\n",
    "            for word in turn[\"text\"].split(\" \"):\n",
    "                word = canonicalize_word(word)\n",
    "                text.append(word)       \n",
    "                for arg in turn['labels']['acts']:\n",
    "                    for d in arg['args']:\n",
    "                        if d['key'] == 'action' and d['val'] == 'book':\n",
    "                            booked=\"True\"\n",
    "        text = text[int(prnct*len(text)):-int(prnct*len(text))]  \n",
    "        if not path.exists(chat_text_file_path+str(prnct)):\n",
    "            os.makedirs(chat_text_file_path+str(prnct))\n",
    "        with open(path.join(chat_text_file_path+str(prnct),chat[\"id\"]+\".\"+booked),\"w\") as f:\n",
    "            f.write(u\" \".join(text).encode('utf-8').strip())\n",
    "\n",
    "prc = 0.4\n",
    "extract_text(prc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1369\n",
      "0.611395178963\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [1 2 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]]\n",
      "[ True False  True False  True  True  True False  True  True False  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True False False False  True  True False\n",
      "  True False  True  True False  True False  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True False False\n",
      " False  True  True  True  True False False  True False  True False  True\n",
      "  True False False  True  True  True False False  True False  True  True\n",
      "  True  True  True False  True  True  True  True  True  True False False\n",
      "  True  True  True False  True False  True  True False False  True False\n",
      " False  True  True  True  True  True False  True False  True  True  True\n",
      "  True  True  True  True  True  True  True False False  True  True  True\n",
      " False  True  True False False  True  True  True  True  True False  True\n",
      "  True False  True False False  True False  True False  True False False\n",
      "  True  True False  True  True  True  True  True False  True  True  True\n",
      "  True False False  True False  True  True  True  True  True  True  True\n",
      "  True  True False False  True False  True  True False  True False  True\n",
      "  True  True False  True  True  True  True  True  True  True False False\n",
      " False  True False False False  True  True  True False  True False  True\n",
      "  True  True False  True  True  True False  True False False  True  True\n",
      "  True False  True  True False False  True  True False  True False  True\n",
      " False False  True  True False False  True  True False  True False False\n",
      "  True  True False False  True False  True  True False  True False  True\n",
      "  True False False False False False  True  True  True False  True  True\n",
      "  True  True  True  True False  True False  True False False  True  True\n",
      "  True  True  True  True  True False False  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True False  True  True\n",
      " False  True False False  True  True False False  True  True  True  True\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False False  True False False  True  True\n",
      "  True False  True False  True False False  True  True  True  True False\n",
      "  True  True  True  True False  True  True  True  True False  True False\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True False False False  True  True  True  True False  True  True\n",
      " False  True  True False False  True  True  True  True  True  True  True\n",
      " False  True False  True  True  True  True False  True False False False\n",
      " False False  True  True  True False  True  True  True  True  True  True\n",
      "  True  True False  True  True False  True False]\n",
      "0.679203539823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "chat_text_file_path= path.join(\"..\",\"frames-data-v1\",\"Frames-dataset\",\"chats_text\")+str(prc)\n",
    "\n",
    "filenames = np.array(os.listdir(chat_text_file_path))\n",
    "\n",
    "filenames_with_path = [os.path.join(chat_text_file_path, fn) for fn in filenames]\n",
    "\n",
    "# tragedies and comedies are coded with 'TR' or 'CO',\n",
    "# e.g., PCorneille_TR-V-1647-Heraclius0001.txt\n",
    "booked = []\n",
    "\n",
    "for fn in filenames:\n",
    "    booked.append(True if fn[-4:]==\"True\" else False)\n",
    "\n",
    "booked = np.array(booked)\n",
    "\n",
    "print len(booked)\n",
    "print booked.sum()/float(len(booked))\n",
    "# .strip() removes the trailing newline '\\n' from each line in the file\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename', min_df=15, max_df=.95, stop_words='english', max_features=3000)\n",
    "\n",
    "dtm = vectorizer.fit_transform(filenames_with_path)\n",
    "\n",
    "dtm = dtm.toarray()\n",
    "\n",
    "print dtm\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dtm, booked, test_size=0.33, random_state=42)\n",
    "\n",
    "logreg = linear_model.LogisticRegression(\n",
    "            multi_class='multinomial', max_iter=128, solver='lbfgs', C=1000000, verbose=1)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "prediction =  logreg.predict(X_test)\n",
    "print y_test == prediction\n",
    "print (y_test == prediction).sum()/ float(len(y_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
