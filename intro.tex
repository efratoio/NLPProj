%!TEX root = main.tex
\section{Introduction}
Dialog understanding has been the subject of 
extensive research \cite{BohusR03,BordesW16,GhazvininejadBC17,ShawarA03}. These works explore 
different aspects of dialogs, from winning an argument \cite{TanNDL16} 
to automatically recognize the emotions conveyed in a conversation at each segment \cite{AyadiKK11}. 
Our project aims to employ NLP and machine learning techniques to 
predict the {\em bottom-line} of a conversation, based on segments 
of it. Such a system can be highly useful in the context of automatic 
chatbots. Consider, for example, a chat with a potential customer where 
from a certain point in the dialog, the system can asses the probability of a purchase 
or what strategy it should employ in order to get there. 
Customer service teams can also benefit from the system as they can decide 
from whether a customer is calling to terminate the contract with the company 
or just to get a discount.

\section{Solution Details}
To predict the bottom-line, our solution will include several stages. 
First, we will identify the topic of the conversation and named entities 
appearing in it. Second, we will segment the conversation, 
and cluster the different segments to identify types of 
``Conversational building blocks'' and learn probabilities 
of transferring from one type of segment to another. 
On top of the segments classification, we will extract features of 
conversation (e.g., dominance of one of the parties, 
the volume of text in the replies, usage of positive/negative words, 
time lapses between responses). 
We will use these features as input to MEMM/RNN to predict the 
most probable continuation of the conversation 
(similarly to what is done with Part-of-Speech tagging). 
\amir{Slava to detail about strategy}


\section{Data}
% \bf{Dialogue Act:}
All the relevant corpora is tagged with Dialogue Act tags - a tag that given to each utterance in the conversation and represents the act that was done by it. For example - Agreement, Question, Anser, etc.

There are several standards of DA - DAMSL, DIT++ etc. 
\begin{itemize}
	\item {\bf The NPS Chat Corpus:} The NPS Chat Corpus, Release 1.0 consists of 10,567 posts out of approximately 500,000 posts gathered from various online chat services.
Each reply is tagged with POS for every word and also a tag that is given to that reply - Accept, Bye, Clarify, Continuer, Emotion, Emphasis, Greet, No Answer, Other, Reject, Statement, System, Wh-Question, Yes Answer, Yes/No Question
	\item {\bf AMI corpus:} The AMI Meeting Corpus is a collection of recordings of meetings based on a scenario. The subject of this scenario is the design of a remote control device in a virtual company.
The correspondence is annotated with dialogue acts and also segments of the conversation are labels with topics, subtopics, and functional labels – 4 types of labels: Opening, Closing, Agenda/Equipment and Chitchat.

	\item {\bf The Dialog Bank:} Contains four dialogue corpora in English and four in Dutch. All corpora is annotated with ISO 24617-2, DIT++ and DAMSL annotations
\end{itemize}

\section{Related Work}
Dialogue analysis studies was first began in the field of linguistics.
John L. Austin introduced the concept of "Speech Act" - he observed that in the act of speaking the meaning of the utterance cannot be determined solely on the facts stated, but there is a notion of action that the speaker perform while uttering that phrase.

The speech act notion was used later on, in order to learn and understand the structure of a conversation, similar to the way parts of speech are used to understand the structure of a sentence. Stolcke \cite{cs-CL-0006023} described algorithms to label conversations with Dialogue Acts, and used it on a large corpora of speech: Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992)
The dialogue is consisting out of turns each turn consists of utterances
The discourse structure annotation Dialogue Act Markup in Several Layers – DAMSL
These DA labels used to build a discourse grammar - statistical model for DA sequences.
The probabilities for this grammar is based on the words transcribed, words recognized acoustics, and prosodic features – acoustics features like pitch, duration, etc.

One on the shortcomings of the Markovian assumption (Not considering global objectives) is that it cannot capture conversational games i.e. the notion that sometimes a new thread is opened in a conversation and then closed. Geertzen \cite{Geertzen} used CFG grammar in order to compete with this issue.

There were some research on graphical methods to derive DA \cite{DBLP:conf/icassp/2005}, and recurrent neural networks \cite{DBLP:conf/coling/1996}. It concluded that factored language models works better then n-gram switching oeverall.