{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"./Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\",\")\n",
    "        d[k] = True if v.strip()==\"True\" else False\n",
    "    return d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns chat dictionary that include the label of each chat, and turns where each turn has sentences list and elapsed \n",
    "#time + the speaker id\n",
    "\n",
    "def gen_chat_data():\n",
    "    chat_path = r\"./Frames-dataset/chats\"\n",
    "    chats = {}\n",
    "    d = build_label_dict()\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        \n",
    "        turns = []\n",
    "        if 'turns' in chat:\n",
    "            tsp = chat['turns'][0]['timestamp']\n",
    "       \n",
    "            for turn in chat['turns']:\n",
    "               \n",
    "                ts = turn['timestamp'] - tsp\n",
    "                tsp = turn['timestamp']\n",
    "                turns.append({\"ti\":ts,\"text\":turn[\"text\"],\"author\":turn[\"author\"]})\n",
    "\n",
    "        chats[filename[:-5]] = {}\n",
    "        chats[filename[:-5]][\"turns\"] = turns\n",
    "        chats[filename[:-5]][\"label\"] = d[filename[:-5]]\n",
    "    return chats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "\n",
    "import functools\n",
    "\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 50\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/efrat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "chats = gen_chat_data()\n",
    "\n",
    "\n",
    "\n",
    "turns = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "\n",
    "for idx in chats.keys():\n",
    "    text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"]])\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    turns.append(sentences)\n",
    "    \n",
    "    labels.append(chats[idx][\"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(turns):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k=k+1      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 6954 unique tokens.\n",
      "Shape of data tensor: (1369, 50, 100)\n",
      "Shape of label tensor: (1369, 2)\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[ 422.  674.]\n",
      "[ 110.  163.]\n",
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(len(data))\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "\n",
    "GLOVE_DIR = \"./data/glove\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "\n",
    "l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_encoder (None, 50, 200)\n",
      "model fitting - Hierachical LSTM\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 50, 100)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 50, 200)       856300      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 200)           240800      timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             402         bidirectional_2[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 1,097,502\n",
      "Trainable params: 1,097,502\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 1096 samples, validate on 273 samples\n",
      "Epoch 1/1\n",
      " 150/1096 [===>..........................] - ETA: 5969s - loss: 1.2136 - acc: 0.5000"
     ]
    }
   ],
   "source": [
    "              \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "print(\"review_encoder %s\"%str(review_encoder._keras_shape))\n",
    "l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "\n",
    "preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "model = Model(review_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),nb_epoch=1, batch_size=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "# print(\"embedded_sequences ndim %d\"%K.ndim(embedded_sequences))\n",
    "\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "# print(\"l_lstm ndim %d\"%K.ndim(l_lstm))\n",
    "\n",
    "l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "# print(\"l_dense ndim %d\"%K.ndim(l_dense))\n",
    "\n",
    "l_att = AttLayer()(l_dense)\n",
    "# print(\"l_att ndim %d\"%K.ndim(l_att))\n",
    "\n",
    "sentEncoder = Model(sentence_input, l_att)\n",
    "print(sentEncoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "# print(\"review_input ndim %d\"%K.ndim(review_input))\n",
    "\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "# print(\"review_encoder ndim %d\"%K.ndim(review_encoder))\n",
    "# print(\"review_encoder %s\"%str(review_encoder._keras_shape))\n",
    "from keras.layers import Reshape\n",
    "#l_reshape = Reshape((50,20000))(review_encoder)\n",
    "\n",
    "l_lstm_sent = Bidirectional(GRU(MAX_SENTS, return_sequences=True))(review_encoder)\n",
    "# l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "# l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "# print(\"l_lstm_sent ndim %d\"%K.ndim(l_lstm_sent))\n",
    "\n",
    "m = Model(review_input,l_lstm_sent)\n",
    "\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "print(\"l_lstm_sent %s\"%str(l_lstm_sent._keras_shape))\n",
    "print(\"l_lstm_sent %d\"%K.ndim(l_lstm_sent))\n",
    "l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "\n",
    "# print(\"l_dense_sent %s\"%str(l_dense_sent._keras_shape))\n",
    "\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "\n",
    "preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "model = Model(review_input, preds)\n",
    "# print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "# x_train.get_value()\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "nb_epoch=10, batch_size=50)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
