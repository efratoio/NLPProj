{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def build_label_dict():\n",
    "    d = {}\n",
    "    label_path = r\"./Frames-dataset/labels.txt\"\n",
    "    label_file = open(label_path, encoding='utf-8')\n",
    "    for line in label_file:\n",
    "        k, v = line.split(\",\")\n",
    "        d[k] = True if v.strip()==\"True\" else False\n",
    "    return d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns chat dictionary that include the label of each chat, and turns where each turn has sentences list and elapsed \n",
    "#time + the speaker id\n",
    "\n",
    "def gen_chat_data():\n",
    "    chat_path = r\"./Frames-dataset/chats\"\n",
    "    chats = {}\n",
    "    d = build_label_dict()\n",
    "    for filename in os.listdir(chat_path):\n",
    "        chat_file = open(os.path.join(chat_path, filename), encoding='utf-8')\n",
    "        chat = json.load(chat_file)\n",
    "        \n",
    "        turns = []\n",
    "        if 'turns' in chat:\n",
    "            tsp = chat['turns'][0]['timestamp']\n",
    "       \n",
    "            for turn in chat['turns']:\n",
    "               \n",
    "                ts = turn['timestamp'] - tsp\n",
    "                tsp = turn['timestamp']\n",
    "                turns.append({\"ti\":ts,\"text\":turn[\"text\"],\"author\":turn[\"author\"]})\n",
    "\n",
    "        chats[filename[:-5]] = {}\n",
    "        chats[filename[:-5]][\"turns\"] = turns\n",
    "        chats[filename[:-5]][\"label\"] = d[filename[:-5]]\n",
    "    return chats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/efrat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "\n",
    "import functools\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 50\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.2\n",
    "GLOVE_DIR = \"./data/glove\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data(chats, l,flag=False,length=0):\n",
    "    texts = []\n",
    "    labels=[]\n",
    "    turns=[]\n",
    "    for idx in chats.keys():\n",
    "        if flag:\n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"][:length]])\n",
    "            \n",
    "        else:\n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"]])\n",
    "        texts.append(text)\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        turns.append(sentences)\n",
    "        labels.append(chats[idx][\"label\"])\n",
    "        \n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    data = np.zeros((l, MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "    for i, sentences in enumerate(turns):\n",
    "        if flag and i==length:\n",
    "            break;\n",
    "        for j, sent in enumerate(sentences):\n",
    "            if j< MAX_SENTS:\n",
    "                wordTokens = text_to_word_sequence(sent)\n",
    "                k=0\n",
    "                for _, word in enumerate(wordTokens):\n",
    "                    if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                        data[i,j,k] = tokenizer.word_index[word]\n",
    "                        k=k+1   \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_texts_labels(chats,flag=False,length=0):\n",
    "    texts=[]\n",
    "    labels=[]\n",
    "    for idx in chats.keys():\n",
    "        if flag:\n",
    "        \n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"][:length]])\n",
    "        else:\n",
    "            text = \"\\n\".join([x[\"text\"] for x in chats[idx][\"turns\"]])\n",
    "        texts.append(text)\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        labels.append(chats[idx][\"label\"])   \n",
    "    return texts,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def prepare_datasets(chats,flag=False, length =0):\n",
    "    if flag:\n",
    "        chats = {k: v for k, v in chats.items() if len(v[\"turns\"])>length}\n",
    "    \n",
    "    texts,labels = create_texts_labels(chats,flag,length)\n",
    "\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    data = create_data(chats,len(texts),flag,length)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "#     data = data[:13]\n",
    "#     labels = labels[:13]\n",
    "    indices = np.arange(len(data))\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "    nb_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "\n",
    "    x_train = data[:-(nb_validation_samples+nb_test_samples)]\n",
    "    y_train = labels[:-(nb_validation_samples+nb_test_samples)]\n",
    "    x_val = data[-(nb_validation_samples+nb_test_samples):-nb_test_samples]\n",
    "    y_val = labels[-(nb_validation_samples+nb_test_samples):-nb_test_samples]\n",
    "    x_test = data[-nb_test_samples:]\n",
    "    y_test = labels[-nb_test_samples:]\n",
    "    \n",
    "    return word_index,x_train, y_train,x_val, y_val,x_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_index():\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_embedding_matrix(word_index):\n",
    "    embeddings_index= create_embedding_index();\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hierarcical_lstm_network(word_index):\n",
    "    embedding_matrix = create_embedding_matrix(word_index)\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SENT_LENGTH,\n",
    "                                trainable=True)\n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "\n",
    "    l_lstm = Bidirectional(LSTM(100))(embedded_sequences)\n",
    "    sentEncoder = Model(sentence_input, l_lstm)\n",
    "    \n",
    "    review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "    review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "    print(\"review_encoder %s\"%str(review_encoder._keras_shape))\n",
    "    l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)\n",
    "\n",
    "    preds = Dense(2, activation='softmax')(l_lstm_sent)\n",
    "    model = Model(review_input, preds)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hierarcical_attention_network(word_index):\n",
    "    embedding_matrix = create_embedding_matrix(word_index)\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "\n",
    "    class AttLayer(Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            self.init = initializations.get('normal')\n",
    "            #self.input_spec = [InputSpec(ndim=3)]\n",
    "            super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            assert len(input_shape)==3\n",
    "            #self.W = self.init((input_shape[-1],1))\n",
    "            self.W = self.init((input_shape[-1],))\n",
    "            #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "            self.trainable_weights = [self.W]\n",
    "            super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "        def call(self, x, mask=None):\n",
    "            eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "            ai = K.exp(eij)\n",
    "            weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "            weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "            return weighted_input.sum(axis=1)\n",
    "\n",
    "        def get_output_shape_for(self, input_shape):\n",
    "            return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    # print(\"embedded_sequences ndim %d\"%K.ndim(embedded_sequences))\n",
    "\n",
    "    l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "    # print(\"l_lstm ndim %d\"%K.ndim(l_lstm))\n",
    "\n",
    "    l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "    # print(\"l_dense ndim %d\"%K.ndim(l_dense))\n",
    "\n",
    "    l_att = AttLayer()(l_dense)\n",
    "    # print(\"l_att ndim %d\"%K.ndim(l_att))\n",
    "\n",
    "    sentEncoder = Model(sentence_input, l_att)\n",
    "    review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "    # print(\"review_input ndim %d\"%K.ndim(review_input))\n",
    "\n",
    "    review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "    # print(\"review_encoder ndim %d\"%K.ndim(review_encoder))\n",
    "    # print(\"review_encoder %s\"%str(review_encoder._keras_shape))\n",
    "    from keras.layers import Reshape\n",
    "    #l_reshape = Reshape((50,20000))(review_encoder)\n",
    "\n",
    "    l_lstm_sent = Bidirectional(GRU(MAX_SENTS, return_sequences=True))(review_encoder)\n",
    "\n",
    "    l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "\n",
    "\n",
    "\n",
    "    l_att_sent = AttLayer()(l_dense_sent)\n",
    "\n",
    "    preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model,x_train, y_train,x_val, y_val,x_test,y_test):\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "    nb_epoch=10, batch_size=1)\n",
    "\n",
    "    return model.evaluate(x_test,y_test,batch_size=1, verbose=1, sample_weight=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def run_net(net,file,word_index,x_train, y_train,x_val, y_val,x_test,y_test):\n",
    "    model = net(word_index)\n",
    "    res = evaluate_model(model,x_train, y_train,x_val, y_val,x_test,y_test)\n",
    "    file.write(str(net)+\"\\n\")\n",
    "    file.write(str(res)+\"\\n\")\n",
    "    \n",
    "def run_networks(network_funcs, configurations,file):\n",
    "\n",
    "    with open(file,\"w\") as results_file:\n",
    "        chats = gen_chat_data()\n",
    "        for net in network_funcs:\n",
    "            \n",
    "            run_net(net,results_file,x_train, y_train,x_val, y_val,x_test,y_test)\n",
    "       \n",
    "        for conf in configurations:\n",
    "            \n",
    "            word_index,x_train, y_train,x_val, y_val,x_test,y_test = prepare_datasets(chats,True,conf)\n",
    "            results_file.write(\"config %d\\n\"%config)\n",
    "            for net in network_funcs:\n",
    "                run_net(net,results_file,word_index,x_train, y_train,x_val, y_val,x_test,y_test)\n",
    "            \n",
    "               \n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3308 unique tokens.\n",
      "Shape of data tensor: (1286, 50, 100)\n",
      "Shape of label tensor: (1286, 2)\n",
      "Total 400000 word vectors.\n",
      "review_encoder (None, 50, 200)\n",
      "Train on 10 samples, validate on 1 samples\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 37s - loss: 1.4414 - acc: 0.2000 - val_loss: 0.5006 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 53s - loss: 0.8642 - acc: 0.3000 - val_loss: 0.4902 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      " 5/10 [==============>...............] - ETA: 22s - loss: 0.6317 - acc: 0.6000"
     ]
    }
   ],
   "source": [
    "run_networks([hierarcical_lstm_network,hierarcical_attention_network],[4,8],\"results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
